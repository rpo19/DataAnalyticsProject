---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

## Imports

```{python}
import pandas as pd
import nltk
import operator
from nltk.tokenize import word_tokenize

from collections import Counter
# #%matplotlib notebook
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns

import pickle
# import igraph as ig
import networkx as nx
from community import generate_dendrogram, community_louvain
from networkx.algorithms.community import greedy_modularity_communities
```

## Data importation

```{python}
data = pd.read_json('../data/products.json', lines=True)
data.head(3)
```

```{python}
# clean numeric values
def cleanProducts():
    data['price'] = data['price'].apply(lambda dictValue : float(list(dictValue.values())[0]) if dictValue != None else float(0))
    data['avg_rating'] = data['avg_rating'].apply(lambda dictValue : float(list(dictValue.values())[0]) if dictValue != None else float(0))
    data['reviews_number'] = data['reviews_number'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
    data['questions_number'] = data['questions_number'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
```

```{python}
cleanProducts()
data.head(3)
```

# Network


## Nodes

```{python}
# export nodes for cytoscape
data['title'] = data['title'].replace(to_replace='[ \t]+', value=' ', regex=True)
data.to_csv("./cytoProducts.csv",columns=["_id", "title", "category", "price", "avg_rating", "reviews_number", "questions_number"], sep="\t", index=False)
```

## Edges

```{python}
def filterEdges(edges, weight):
    return edges.loc[edges['weight'] in weight]
```

```{python}
# create complete list of edges
edges = []
for i, row in data.iterrows():
    for subrow in row['bought_together']:
        ft = {'from': row['_id'], 'to': subrow, 'weight': 3}
        edges.append(ft)
    for subrow in row['also_bought']:
        ft = {'from': row['_id'], 'to': subrow, 'weight': 2}
        edges.append(ft)
    for subrow in row['also_viewed']:
        ft = {'from': row['_id'], 'to': subrow, 'weight': 1}
        edges.append(ft)

edges = pd.DataFrame(edges)
# keep a copy the complete list... maybe it will be useful in future
all_edges = pd.DataFrame(edges)

nodes = data['_id']
print("# edges:", len(edges))
print("# nodes:", len(nodes))
```

```{python}
# remove edges with not available products
for i, edge in edges.iterrows():
    if edge['to'] in data['_id'].values:
        continue
    else:
        edges = edges.drop(i)

print("# edges:", len(edges))
```

```{python}
# remove also_viewed and merge also_bought with bought_together
edges = all_edges.loc[all_edges['weight'] != 1][['from','to']]
print("# edges:", len(edges))
edges
```

```{python}
# remove symmetries
edges = pd.DataFrame.from_records(list({tuple(sorted(item)) for item in edges.values}), columns=['from', 'to'])
edges = edges.drop_duplicates(subset=["from", "to"])

print("# edges:", len(edges))
```

```{python}
# export edges for cytoscape
edges.to_csv("./cytoEdges.csv", sep="\t", columns=["from", "to"], index=False)
```

# Network analysis


## Create full network

```{python}
edges = pd.read_csv('../networkData/cytoEdges.csv', sep='\t')
edges
```

```{python}
full_graph = nx.from_pandas_edgelist(edges, source='from', target='to')
```

## Filter products by categories

```{python}
categories = data['category'].value_counts()
print("number of categories: {}".format(len(categories)))
print(categories)
categories_dict = categories.to_dict()
categories_lbl = list(categories.keys())
```

## Create subgraph  by category

```{python}
products_by_cat = data.loc[data['category'] == 'musical-instruments']['_id'].values
subgraph_cat = full_graph.subgraph(products_by_cat)
```

## Compute centrality

```{python}
degree_centrality = nx.degree_centrality(subgraph_cat)
degree_centrality = dict(sorted(degree_centrality.items(), key=operator.itemgetter(1),reverse=True))
degree_centrality
```

```{python}
degrees_dict = dict(subgraph_cat.degree)
degrees_dict = dict(sorted(degrees_dict.items(), key=operator.itemgetter(1),reverse=True))
degrees_dict
```

```{python}
betweenness_centrality = nx.betweenness_centrality(subgraph_cat)
betweenness_centrality = dict(sorted(betweenness_centrality.items(), key=operator.itemgetter(1),reverse=True))
betweenness_centrality
```

```{python}
closeness_centrality = nx.closeness_centrality(subgraph_cat)
closeness_centrality = dict(sorted(closeness_centrality.items(), key=operator.itemgetter(1),reverse=True))
closeness_centrality
```

```{python}
eigenvector_centrality = nx.eigenvector_centrality(subgraph_cat, max_iter=1000)
eigenvector_centrality = dict(sorted(eigenvector_centrality.items(), key=operator.itemgetter(1),reverse=True))
eigenvector_centrality
```

## Create a network for each relation (OLD)

```{python}
# network for bought_together
edges_bt = filterEdges(all_edges, 3)
G_bt = nx.from_pandas_edgelist(edges_bt, source='from', target='to')
print("### bought_together ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_bt))
print()
# network for also_bought
edges_ab = filterEdges(all_edges, 2)
G_ab = nx.from_pandas_edgelist(edges_ab, source='from', target='to')
print("### also_bought ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_ab))
print()

# network for also_viewed
edges_av = filterEdges(all_edges, 1)
G_av = nx.from_pandas_edgelist(edges_av, source='from', target='to')
print("### also_viewed ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_av))
print()


```

## Communities


### Utils

```{python}
# communities detection
def getCommunities(network):
    return sorted(greedy_modularity_communities(network), key=len, reverse=True)
    
def getCommunitiesLouvein(network):
    return community_louvain.best_partition(network)

def getDominantCategory(categories, threshold):
    tot = sum(categories)
    max_val = threshold*tot
    max_cat = None
    for key, value in categories.items():
        if value >= max_val:
            max_val = value
            max_cat = key
    return max_cat 

def getTitlesFromIDs(ids):
    return data.loc[data['_id'].isin(ids)]['title']
```

### Single relations (OLD)

```{python}
communities_bt = getCommunities(G_bt)
print("# communities bt:", len(communities_bt))

communities_ab = getCommunities(G_ab)
print("# communities ab:", len(communities_ab))

communities_av = getCommunities(G_av)
print("# communities av:", len(communities_av))

```

```{python}
# get communities with louvein
partition = getCommunitiesLouvein(G_ab)
```

```{python}
print(len(partition))
partition_df = pd.DataFrame(partition.items(), columns=['_id', 'community'])

grouped_df = partition_df.groupby('community').count()
print(len(grouped_df))
grouped_df.sort_values('_id').loc[grouped_df['_id'] >= 5]
```

#### bt examples

```{python}
# products in a community

communities_bt_list =[list(x) for x in communities_bt]
community_bt_x = communities_bt_list[3]
community_bt_x
print(data.loc[data['_id'].isin(community_bt_x)].groupby('category').count())

```

```{python}
data.loc[data['_id'].isin(community_bt_x)]
```

#### also viewed examples

```{python}
communities_av_list =[list(x) for x in communities_av]

community_av_1 = communities_av_list[4]
community_av_1
print(data.loc[data['_id'].isin(community_av_1)].groupby('category').count())
data.loc[data['_id'].isin(community_av_1)]
```

#### predominant category for each community

```{python}
communities_bt = [list(x) for x in getCommunities(G_bt)]

```

```{python}
#print(communities_bt)
communities_bt_df = []
#pd.DataFrame(columns=["community", "category"])
for community in communities_bt:
    categories = data.loc[data['_id'].isin(community)].groupby('category').count()['_id']
    category = getDominantCategory(categories, 0.75)
    communities_bt_df.append({"community": community,"category": category})
communities_bt_df = pd.DataFrame(communities_bt_df, columns=['community', 'category'])

```

```{python}
communities_bt_df.loc[communities_bt_df['category'] == 'videogames']
```

```{python}
print(getTitlesFromIDs(communities_bt_df.iloc[4]['community']).to_string())
print("___________")
print(getTitlesFromIDs(communities_bt_df.iloc[10]['community']).to_string())
print("___________")
print(getTitlesFromIDs(communities_bt_df.iloc[33]['community']).to_string())

```

### Most common words (title) for each community
Important where category==None

```{python}

```

## Plot

```{python}
def set_node_community(G, communities):
        '''Add community to node attributes'''
        for c, v_c in enumerate(communities):
            for v in v_c:
                # Add 1 to save 0 for external edges
                G.nodes[v]['community'] = c + 1
                
def get_color(i, r_off=1, g_off=1, b_off=1):
        '''Assign a color to a vertex.'''
        r0, g0, b0 = 0, 0, 0
        n = 16
        low, high = 0.1, 0.9
        span = high - low
        r = low + span * (((i + r_off) * 3) % n) / (n - 1)
        g = low + span * (((i + g_off) * 5) % n) / (n - 1)
        b = low + span * (((i + b_off) * 7) % n) / (n - 1)
        return (r, g, b)

def getNodeColors(graph):
    return [get_color(graph.nodes[v]['community']) for v in graph.nodes]

def readPos(fileName):
    in_file = open(fileName,"rb")
    text = in_file.read()
    in_file.close()
    return pickle.loads(text)

def writePos(pos, fileName):
    out_file = open(fileName,"wb")
    out_file.write(pickle.dumps(pos))
    out_file.close()

def plotNetworkWithExistingPos(graph, fileName, node_size=300, figsize=(100,100)):
    pos = readPos(fileName)
    plt.figure(figsize=figsize)
    node_color = getNodeColors(graph)
    nx.draw_networkx(graph, pos=pos, with_labels = False, node_color=node_color, \
                     edge_color="black", node_size=node_size)

def plotNetworkNew(graph, fileName):
    pos = nx.spring_layout(graph, k=0.1)
    writePos(pos, fileName)
    plt.figure(figsize=(100,100))
    node_color = getNodeColors(graph)
    # vedere se aggiungere nodelist=nodes[category==...] per fare solo categories piu importanti
    nx.draw_networkx(graph, pos=pos, with_labels = False, node_color=node_color)
```

```{python}
set_node_community(G_bt, communities_bt)
plotNetworkWithExistingPos(G_bt, 'pos.txt', figsize=(50,50))
```

```{python}
def filterone(G, clist):
    res = [v for v in G_bt.nodes if G_bt.nodes[v]["community"] in clist]
    return res
```

```{python}
filtered = filterone(G_bt, [1,2,3])
len(filtered)
```

```{python}
G_filtered=nx.Graph()
```

```{python}
G_filtered.add_nodes_from(filtered)
G_filtered.nodes
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1])), 'pos.txt', node_size=80, figsize=(4,4))
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1,2])), 'pos.txt', node_size=80, figsize=(4,4))
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1,2,3])), 'pos.txt', node_size=80, figsize=(10,10))
```

```{python}
# plot partitions generated with louvein

# color the nodes according to their partition
#plt.figure(figsize=(50,50))
#cmap = cm.get_cmap('viridis', max(partition.values()) + 1)
#nx.draw_networkx_nodes(G_bt, pos, partition.keys(), node_size=40,
                       cmap=cmap, node_color=list(partition.values()))
#nx.draw_networkx_edges(G_bt, pos, alpha=0.5)
#plt.show()
```

```{python}
# plotNetworkWithExistingPos(G_bt.subgraph(partition), 'pos.txt', node_size=80, figsize=(50,50))
```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```
