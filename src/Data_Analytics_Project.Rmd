---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

## Imports

```{python}
# !pip install --upgrade -r requirements.txt
```

```{python}
import pandas as pd
import string
import nltk
import operator
import scipy
import json
import random

from random import randint
from tqdm.notebook import tqdm
tqdm.pandas()
import statistics

from collections import Counter
# #%matplotlib notebook
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import numpy as np

import pickle
# import igraph as ig
import networkx as nx
from community import generate_dendrogram, community_louvain
from networkx.algorithms.community import greedy_modularity_communities

from wordcloud import WordCloud
from pandas.plotting import scatter_matrix
```

## Data importation

```{python}
data = pd.read_json('../data/products.json', lines=True)
data
```

```{python}
# clean numeric values
def cleanProducts():
    data['price'] = data['price'].apply(lambda dictValue : float(list(dictValue.values())[0]) if dictValue != None else float(0))
    data['avg_rating'] = data['avg_rating'].apply(lambda dictValue : float(list(dictValue.values())[0]) if dictValue != None else float(0))
    data['reviews_number'] = data['reviews_number'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
    data['questions_number'] = data['questions_number'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
```

```{python}
cleanProducts()
data.head(3)
```

# Network


## Utils

```{python}
def filterEdges(edges, weight):
    return edges.loc[edges['weight'] in weight]

def csvCreateNodes():
    # export nodes for cytoscape
    data['title'] = data['title'].replace(to_replace='[ \t]+', value=' ', regex=True)
    data.to_csv("../networkData/cytoProducts.csv",columns=["_id", "title", "category", "price", "avg_rating", "reviews_number", "questions_number", 'community'], sep="\t", index=False)

def csvCreateEdges():        
    # create complete list of edges
    edges = []
    for i, row in data.iterrows():
        for subrow in row['bought_together']:
            ft = {'from': row['_id'], 'to': subrow, 'weight': 3}
            edges.append(ft)
        for subrow in row['also_bought']:
            ft = {'from': row['_id'], 'to': subrow, 'weight': 2}
            edges.append(ft)
        for subrow in row['also_viewed']:
            ft = {'from': row['_id'], 'to': subrow, 'weight': 1}
            edges.append(ft)

    edges = pd.DataFrame(edges)
    # keep a copy the complete list... maybe it will be useful in future
    all_edges = pd.DataFrame(edges)

    nodes = data['_id']
    print("# edges:", len(edges))
    print("# nodes:", len(nodes))
    
    # remove also_viewed and merge also_bought with bought_together
    edges = all_edges.loc[all_edges['weight'] != 1][['from','to']]
    print("# edges:", len(edges))
    edges
    
    # remove edges with not available products
    for i, edge in edges.iterrows():
        if edge['to'] in data['_id'].values:
            continue
        else:
            edges = edges.drop(i)

    print("# edges:", len(edges))
    
    
    # remove symmetries
    edges = pd.DataFrame.from_records(list({tuple(sorted(item)) for item in edges.values}), columns=['from', 'to'])
    edges = edges.drop_duplicates(subset=["from", "to"])

    print("# edges:", len(edges))
    
    # export edges for cytoscape
    #edges.to_csv("../networkData/cytoEdges.csv", sep="\t", columns=["from", "to"], index=False)
```

```{python}
csvCreateEdges()
```

# Network analysis


## Read/Create edges

```{python}
for i in range(1,3):
    try:
        edges = pd.read_csv('../networkData/cytoEdges.csv', sep='\t')
        break
    except:
        csvCreateEdges()
edges
```

## Create full network

```{python}
full_graph = nx.from_pandas_edgelist(edges, source='from', target='to')
```

```{python}
giant = full_graph.subgraph(list(max(nx.connected_components(full_graph), key=len)))
```

```{python}
statistics.mean(list(dict(list(nx.degree(full_graph))).values()))
```

```{python}
nx.average_clustering(full_graph)
```

```{python}
nx.density(full_graph)
```

```{python}
nx.degree_assortativity_coefficient(full_graph)
```

```{python}
matrix = nx.degree_mixing_matrix(giant)
```

```{python}

```

```{python}
plt.figure(figsize=(10,10))
plt.imshow(matrix)
plt.gca().invert_yaxis()
plt.colorbar()
```

```{python}
degree_cent = nx.degree_centrality(full_graph)
betweenness_cent = nx.betweenness_centrality(full_graph)
closeness_cent = nx.closeness_centrality(full_graph)
eigen_cent = nx.eigenvector_centrality(full_graph)
```

```{python}
centralitiesFullGraph = pd.DataFrame({'_id': list(degree_cent.keys()), 'degree': list(degree_cent.values()), \
              'betweenness': list(betweenness_cent.values()), \
             'closeness_cent': list(closeness_cent.values()), \
             'eigen_cent': list(eigen_cent.values())})
```

```{python}
centralitiesFullGraph.to_csv('../networkData/centrailitiesFullGraph.csv')
```

```{python}
filteredDataCent = data.loc[data['_id'].isin(centralitiesFullGraph.sort_values('degree', ascending=False).head(5)['id'].values)]
```

```{python}
pd.merge(filteredDataCent[['_id', 'title']], centralitiesFullGraph, on='_id').sort_values('degree', ascending=False).to_csv('../networkData/degreeFullGraph.csv', index=False, sep='\t')
```

```{python}
pd.merge(filteredDataCent[['_id', 'title']], centralitiesFullGraph, on='_id').sort_values('closeness_cent', ascending=False).to_csv('../networkData/closenessFullGraph.csv', index=False, sep='\t')
```

```{python}
centralitiesFullGraph.sort_values('degree', ascending=False).head(5)

```

```{python}
centralitiesFullGraph.sort_values('betweenness', ascending=False).head(5)
```

```{python}
centralitiesFullGraph.sort_values('closeness_cent', ascending=False).head(5)
```

```{python}
centralitiesFullGraph.sort_values('eigen_cent', ascending=False).head(5)
```

```{python}
data.loc[data['_id'] == 'B0792HCFTG']
```

```{python}
connected_components = list(nx.connected_components(full_graph))
connected_components 
```

```{python}
[len(c) for c in sorted(connected_components, key=len, reverse=True)]
```

## Read/Create nodes

```{python}
data = pd.DataFrame(data.loc[data['_id'].isin(list(full_graph))])
len(data)
```

```{python}

```

## Filter products by categories

```{python}
categories = data['category'].value_counts()
print("number of categories: {}".format(len(categories)))
print(categories)
categories_dict = categories.to_dict()
categories_lbl = list(categories.keys())
```

## Example: centralities of a category

```{python}
products_by_cat = data.loc[data['category'] == 'musical-instruments']['_id'].values
subgraph_cat = full_graph.subgraph(products_by_cat)
degree_centrality = nx.degree_centrality(subgraph_cat)
degree_centrality = dict(sorted(degree_centrality.items(), key=operator.itemgetter(1),reverse=True))
degrees_dict = dict(subgraph_cat.degree)
degrees_dict = dict(sorted(degrees_dict.items(), key=operator.itemgetter(1),reverse=True))
betweenness_centrality = nx.betweenness_centrality(subgraph_cat)
betweenness_centrality = dict(sorted(betweenness_centrality.items(), key=operator.itemgetter(1),reverse=True))
closeness_centrality = nx.closeness_centrality(subgraph_cat)
closeness_centrality = dict(sorted(closeness_centrality.items(), key=operator.itemgetter(1),reverse=True))
eigenvector_centrality = nx.eigenvector_centrality(subgraph_cat, max_iter=1000)
eigenvector_centrality = dict(sorted(eigenvector_centrality.items(), key=operator.itemgetter(1),reverse=True))
```

## Compute stats for each category

```{python}
def computeStats(data, graph):
    subgraph = graph.subgraph(data)
    # cardinality
    cardinality = len(data)
    # degree centrality
    degree_centrality = nx.degree_centrality(subgraph)
    degree_centrality = dict(sorted(degree_centrality.items(), key=operator.itemgetter(1),reverse=True))
    max_degree_id = list(degree_centrality.keys())[0]
    # degree
    degrees = dict(subgraph.degree)
    degrees = dict(sorted(degrees.items(), key=operator.itemgetter(1),reverse=True))
    max_degree = list(degrees.values())[0]
    # TODO (maybe): most positive/negative products by sentiment
    # TODO (maybe): reviews number
    return cardinality, max_degree_id, max_degree

          
```

```{python}
categories_stats = pd.DataFrame(columns = ['category','cardinality','max_degree_id', 'max_degree', 'most_positive_id'])

for category in categories_lbl:
    data_by_category = data.loc[data['category'] == category]['_id'].values
    cardinality, max_degree_id, max_degree = computeStats(data_by_category, full_graph)
    categories_stats = categories_stats.append({'category':category,'cardinality':cardinality, 'max_degree_id':max_degree_id,\
                             'max_degree':max_degree}, ignore_index = True)
    
categories_stats
```

```{python}
categories_stats.drop('most_positive_id', axis=1).head(5).to_csv('../networkData/categoriesStats.csv', index=False)
pd.merge(categories_stats.drop('most_positive_id', axis=1).head(5), data[['_id', 'title']], left_on='max_degree_id', right_on='_id').to_csv('../networkData/categoriesStats.csv', sep='\t')
```

```{python}
import statistics
statistics.median(data.loc[data["_id"].isin([cat["max_degree_id"] for _,cat in categories_stats.iterrows()])]["avg_rating"])
#statistics.mean(data.loc[data["_id"].isin([cat["max_degree_id"] for _,cat in categories_stats.iterrows()])]["avg_rating"])
```

```{python}
np.seterr(all='ignore')
#sns.distplot(data.loc[data["_id"].isin([cat["max_degree_id"] for _,cat in categories_stats.iterrows()])]["avg_rating"])
np.average(data["avg_rating"]*(data["reviews_number"]/np.average(data["reviews_number"])))
```

```{python}
np.average(data["avg_rating"].values, weights=data["reviews_number"].values)
#np.average(data["avg_rating"])
```

```{python}
miniworld = data.loc[data["_id"].isin([cat["max_degree_id"] \
                                      for _,cat in categories_stats.iterrows()])]
np.average(miniworld["avg_rating"].values, weights = miniworld["reviews_number"].values)
```

```{python}
np.seterr(all='ignore')
d = data.loc[data["_id"].isin([cat["max_degree_id"] for _,cat in categories_stats.iterrows()])]["questions_number"]
sns.distplot(d)
sns.distplot(data["questions_number"]) 
```

```{python}
np.seterr(all='ignore')
sns.distplot(data.loc[data["_id"].isin([cat["max_degree_id"] \
                                        for _,cat in categories_stats.iterrows()])]["price"])
#sns.distplot(d, kde=False)
sns.distplot(data["price"])
```

```{python}
data
```

```{python}
print(type(categories_stats))
for _,row in categories_stats.iterrows():
    print(row)
    print(row["category"])
    break
```

## Communities detection


### Utils

```{python}
# communities detection
def getCommunities(graph):
    return sorted(greedy_modularity_communities(graph), key=len, reverse=True)
    
def getCommunitiesLouvein(network):
    return community_louvain.best_partition(network)

np.seterr(all='raise')
def getCategoryStatZ(categories, threshold, top):
    try:
        z = scipy.stats.zscore(categories)
    except FloatingPointError:
        if len(categories) == 1:
            max_cat = categories.keys()[0]
            # distrib = 1, zscore = 0
            distrib = {max_cat: (np.float64(1.0),np.float64(0.0))}
        else:
            # all categories same wheight
            max_cat = None
            distrib = dict(zip(categories.head(top).keys(), zip(categories.values/sum(categories), np.zeros(len(categories)))))
        return max_cat,distrib

    max_cat = categories.keys()[z.argmax()] if z.max() > threshold else None
    distrib = categories[z.argsort()[::-1]].head(top)
    sum_values = sum(distrib.values)
    z[::-1].sort()
    distrib = dict((ki, (di, zi)) for ki,di,zi in zip(distrib.keys(), distrib.values/sum_values, z))
    return max_cat, distrib

def getCategoryStats(categories, threshold, top):  
    categories = categories.sort_values(ascending=False)
    tot = sum(categories)
    max_val = threshold*tot
    max_cat = None
    #distrib = pd.DataFrame(columns = ['category','value'])
    distrib = {}
    for key, value in categories.items():
        # dominant category
        if value >= max_val:
            max_val = value
            max_cat = key
        # categories distribution
        if key in categories.head(top):
            #distrib = distrib.append({'category':key, 'value': value}, ignore_index = True)
            distrib[key] = value
    
    # categories distribution
    others_value = sum(categories.iloc[top:len(categories)])
    #distrib.append({'category': 'others', 'value': others_value}, ignore_index = True)
    distrib['others'] = others_value
    return max_cat, distrib

def getTitlesFromIDs(ids):
    return data.loc[data['_id'].isin(ids)]['title']
```

### Communities analysis

```{python}
communities = getCommunities(full_graph)
print("# communities:", len(communities))
```

```{python}
communities = [list(x) for x in communities]
```

```{python}
communities_stats = pd.DataFrame(columns = ['id','community', 'dominant_category', 'cardinality', 'max_degree_id', 'max_degree', 'avg_clust', 'top_words'])
for i, community in enumerate(communities):
    # dominant category
    categories = data.loc[data['_id'].isin(community)].groupby('category').count()['_id']
    #dominant_category, categories_distribution = getCategoryStats(categories, 0.75, 3)
    dominant_category, categories_distribution = getCategoryStatZ(categories, 1.5, 3)
    # compute stats
    cardinality, max_degree_id, max_degree = computeStats(community, full_graph)
    avg_clust = nx.average_clustering(full_graph.subgraph(community))
    communities_stats = communities_stats.append({'id': i+1,'community': community,'dominant_category': dominant_category,\
                                                  'categories_distribution':categories_distribution,'cardinality':cardinality,\
                                                  'max_degree_id':max_degree_id, 'max_degree':max_degree, 'avg_clust': avg_clust}, ignore_index = True)
communities_stats
```

## Average clustering coefficient 

```{python}

np.average(communities_stats['avg_clust'].values, weights=communities_stats['cardinality'].values)
```

## Assign community to each product

```{python}
for _, community in communities_stats.iterrows():
    rows = list(data.index[data['_id'].isin(community['community'])])
    data.loc[rows, 'community'] = int(community['id'])
data
```

### Write cytoscape products csv

```{python}
csvCreateNodes()
```

## Most common words for each community

```{python}
nltk.download('punkt')
nltk.download('stopwords')
#nltk.download('averaged_perceptron_tagger')
#nltk.download('maxent_ne_chunker')
#nltk.download('words')
```

```{python}
#communities_stats['category'].count()
#communities_stats.loc[communities_stats['dominant_category'].isnull()].sort_values('cardinality', ascending=False)['categories_distribution'].values
#type(communities_stats.iloc[1])
#communities_stats
stop_words = nltk.corpus.stopwords.words('italian') + nltk.corpus.stopwords.words('english')
punctuation = string.punctuation + "-"

def flatten(listoflists):
    return [item for list in listoflists for item in list]

import spacy
nlp = spacy.load("it_core_news_sm")
def processTitlesSpacy(titles):
    ret = titles.apply(lambda x: [ent.text for ent in nlp(x).ents])
    ret = Counter(flatten(ret))
    ret = dict(ret.most_common(50))
    return ret

def processTitles(titles):
    ret = titles.apply(nltk.tokenize.word_tokenize)
    
    ret = ret.apply(lambda x: [word for word in x if len(word)>2 and \
                               word.lower() not in punctuation])
    #ret = ret.apply(lambda x: [ word.replace(',', '') for word in x])
    ret = ret.apply(nltk.pos_tag)
    ret = ret.apply(nltk.ne_chunk)
    ret = ret.apply(lambda x : [w[0] if isinstance(w, tuple) else " ".join(t[0] for t in w) for w in x])
    ret = ret.apply(lambda x: [word.lower() for word in x if len(word)>2 and word.lower() not in stop_words])
    ret = Counter(flatten(ret))
    ret = dict(ret.most_common(50))
    return ret

def processTitlesSimple(titles):
    ret = titles.apply(nltk.tokenize.word_tokenize)
    ret = ret.apply(lambda x: [word.lower() for word in x if len(word)>2 and word.lower() \
                               not in punctuation and word.lower() not in stop_words])
    ret = Counter(flatten(ret))
    return dict(ret.most_common(50))
```

```{python}
method = "simple"
if method == "ne":
    processTitlesFun=processTitles
elif method == "spacy":
    processTitlesFun=processTitlesSpacy
else:
    processTitlesFun=processTitlesSimple

communities_stats["top_words"] = communities_stats["community"].apply(\
                                        lambda x: processTitlesSimple(data.loc[data['_id'].isin(x)]["title"]))

communities_stats["top_ents"] = communities_stats["community"].apply(\
                                        lambda x: processTitlesSpacy(data.loc[data['_id'].isin(x)]["title"]))

```

```{python}
import base64
def word_cloud_to_base64(word_counter):
    wordcloud = WordCloud(background_color='white').generate_from_frequencies(word_counter)
    filename = "/tmp/wordcloud.png"
    wordcloud.to_file(filename)
    with open(filename, "rb") as wfile:
        encoded = base64.b64encode(wfile.read()).decode('ascii')
    return encoded
```

```{python}
communities_stats["wordclouds"] = communities_stats.iloc[0:9]["top_words"].apply(word_cloud_to_base64)
```

```{python}
communities_stats.iloc[0:11]["wordclouds"]
```

```{python}
communities_stats.to_csv("../dataApp/communities_stats.csv", sep='\t')
```

```{python}
data.loc[data['_id'].isin(communities_stats.iloc[0]["community"])]["title"].values
```

```{python}
from tqdm.notebook import tqdm
tqdm.pandas()
communities_stats.progress_apply(lambda x: x)
```

```{python}
communities_stats["top_words"]
```

```{python}
titles = data.loc[data['_id'].isin(communities_stats.iloc[0]["community"])]["title"]
titles
p = processTitles(titles)
p
```

```{python}
d
```

```{python}
nltk.ne_chunk(nltk.pos_tag("il più grosso è Christian Bernasconi".split(" ")))
```

```{python}
import spacy

nlp = spacy.load("it_core_news_sm")
#[ent for ent in nlp("Mary Poppins").ents]
doc = nlp("Playstation 4")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
```

```{python}
nee = titles.apply(lambda x: [ent.text for ent in nlp(x).ents])

```

```{python}
c = Counter(flatten(nee))
```

```{python}
c.most_common(20)
```

```{python}
titles
```

```{python}
str = "ciao mi chiamo Capitan Harlock e sono Blu"
a = nltk.tokenize.word_tokenize(str)
a = nltk.pos_tag(a)
a = nltk.ne_chunk(a)
[ w[0] if isinstance(w, tuple) else " ".join(t[0] for t in w) for w in a]
```

```{python}
" ".join([w for w,_ in f.leaves()])
```

```{python}
merged_titles = " ".join([data.loc[data['_id'] == product_id].iloc[0]["title"] for product_id in communities_stats.iloc[53]["community"]])
r = processMergedTitles(merged_titles)
r
```

```{python}
print(type(communities_stats.loc[communities_stats["id"] == 1].iloc[0]["top_words"]))
print(type(communities_stats.loc[250]["top_words"]))
```

```{python}
[ww for wc in communities_stats["top_words"].values for ww in wc if len(ww.split(" "))>1]
#print([ww for ww in c if len(ww.split(" "))>1])
```

```{python}
communities_stats.loc[communities_stats["dominant_category"].isnull()]
```

```{python}
c = communities_stats.iloc[15]
print(c)
wordcloud = WordCloud(background_color='white').generate_from_frequencies(c["top_ents"])
plt.figure(figsize = (10, 8), facecolor = None) 
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
```

```{python}
communities_stats.iloc[10]
```

```{python}
comm = communities_stats.iloc[0]
idprice = [data.loc[data['_id'].isin(comm["community"])].iloc[0][["_id","price"]] for product_id ]
idprice
```

```{python}
comm = communities_stats.iloc[8]
print(comm[["dominant_category", "top_words", "categories_distribution"]])
d = data.loc[data['_id'].isin(comm["community"])]
z = scipy.stats.zscore(d["price"])
dc = d.copy()
dc["zscore"] = z
c = z.argsort()[::-1]
dc.iloc[c]
```

```{python}
comm = communities_stats.iloc[3]
print(comm)
data.loc[data['_id'] == comm["max_degree_id"]]
```

```{python}
[data.loc[data['_id'] == product_id].iloc[0]["title"] for product_id in communities_stats.iloc[54]["community"]]
```

```{python}
print(r)
```

```{python}
print(dict(r.most_common(10)))
print(len(r))
```

```{python}
wordcloud = WordCloud().generate_from_frequencies(dict(r.most_common(50)))
plt.figure(figsize = (10, 8), facecolor = None) 
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
```

```{python}
communities_stats.iloc[0:20][["top_words", "dominant_category"]]
```

```{python}
communities_stats.loc[communities_stats["dominant_category"].isnull()]["categories_distribution"]
```

```{python}
prova = data.loc[data['_id'].isin(communities[0])].groupby('category').count()['_id']
top = 3

sum(prova.sort_values(ascending=False).head(top))+sum(prova.sort_values(ascending=False).iloc[top:len(prova)])

```

```{python}
np.seterr(all='ignore')
sns.distplot(data.loc[data["_id"].isin([community["max_degree_id"] \
                           for _,community in communities_stats.iterrows()])]["avg_rating"])
#sns.distplot(d, kde=False)
sns.distplot(data["avg_rating"])
```

```{python}
np.seterr(all='ignore')
sns.distplot(data.loc[data["_id"].isin([community["max_degree_id"] \
                           for _,community in communities_stats.iterrows()])]["price"])
#sns.distplot(d, kde=False)
sns.distplot(data["price"])
```

```{python}
data
```

## Bridges

```{python}
bridges = list(nx.bridges(full_graph))
```

```{python}
bridges_cat = pd.DataFrame(columns=['id_from', 'id_to', 'cat_from', 'cat_to'])
for edge in bridges:
    cat_from = data.loc[data['_id'] == edge[0]]['category'].values[0]
    cat_to = data.loc[data['_id'] == edge[1]]['category'].values[0]
    if cat_from != cat_to:
        bridges_cat = bridges_cat.append({'id_from': edge[0], 'id_to': edge[1], 'cat_from': cat_from ,\
                                          'cat_to': cat_to}, ignore_index=True)
bridges_cat
```

```{python}
bridges_betweenness = nx.edge_betweenness_centrality(full_graph)
```

```{python}
bridges_cat = pd.DataFrame(columns=['id_from', 'id_to', 'cat_from', 'cat_to', 'edge_betweenness'])
for edge in bridges:
    cat_from = data.loc[data['_id'] == edge[0]]['category'].values[0]
    cat_to = data.loc[data['_id'] == edge[1]]['category'].values[0]
    if cat_from != cat_to:
        bridges_cat = bridges_cat.append({'id_from': edge[0], 'id_to': edge[1], 'cat_from': cat_from ,\
                                          'cat_to': cat_to, 'edge_betweenness': bridges_betweenness[edge]}, ignore_index=True)
bridges_cat
```

```{python}
bridges_cat.to_csv('./bridges.csv',sep="\t", index=False)
```

## ! read csv !

```{python}
bridges_cat = pd.read_csv('./bridges.csv', sep='\t')
```

```{python}
cat_in_bridges = bridges_cat[['cat_from','cat_to']].drop_duplicates().values
bridges_stats = pd.DataFrame()
for cat in cat_in_bridges:
    filtered_bridges = bridges_cat.loc[(bridges_cat['cat_from']==cat[0]) & (bridges_cat['cat_to']==cat[1])]
    bridges_stats = bridges_stats.append(filtered_bridges.loc[filtered_bridges['edge_betweenness'] == max(filtered_bridges['edge_betweenness'])].head(1), ignore_index=True)
bridges_stats

```

## Create a network for each relation (OLD)

```{python}
# network for bought_together
edges_bt = filterEdges(all_edges, 3)
G_bt = nx.from_pandas_edgelist(edges_bt, source='from', target='to')
print("### bought_together ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_bt))
print()
# network for also_bought
edges_ab = filterEdges(all_edges, 2)
G_ab = nx.from_pandas_edgelist(edges_ab, source='from', target='to')
print("### also_bought ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_ab))
print()

# network for also_viewed
edges_av = filterEdges(all_edges, 1)
G_av = nx.from_pandas_edgelist(edges_av, source='from', target='to')
print("### also_viewed ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_av))
print()


```

## Communities


### Single relations (OLD)

```{python}
communities_bt = getCommunities(G_bt)
print("# communities bt:", len(communities_bt))

communities_ab = getCommunities(G_ab)
print("# communities ab:", len(communities_ab))

communities_av = getCommunities(G_av)
print("# communities av:", len(communities_av))

```

```{python}
# get communities with louvein
partition = getCommunitiesLouvein(G_ab)
```

```{python}
print(len(partition))
partition_df = pd.DataFrame(partition.items(), columns=['_id', 'community'])

grouped_df = partition_df.groupby('community').count()
print(len(grouped_df))
grouped_df.sort_values('_id').loc[grouped_df['_id'] >= 5]
```

#### bt examples

```{python}
# products in a community

communities_bt_list =[list(x) for x in communities_bt]
community_bt_x = communities_bt_list[3]
community_bt_x
print(data.loc[data['_id'].isin(community_bt_x)].groupby('category').count())

```

```{python}
data.loc[data['_id'].isin(community_bt_x)]
```

#### also viewed examples

```{python}
communities_av_list =[list(x) for x in communities_av]

community_av_1 = communities_av_list[4]
community_av_1
print(data.loc[data['_id'].isin(community_av_1)].groupby('category').count())
data.loc[data['_id'].isin(community_av_1)]
```

#### predominant category for each community

```{python}
communities_bt = [list(x) for x in getCommunities(G_bt)]

```

```{python}
#print(communities_bt)
communities_bt_df = []
#pd.DataFrame(columns=["community", "category"])
for community in communities_bt:
    categories = data.loc[data['_id'].isin(community)].groupby('category').count()['_id']
    category = getDominantCategory(categories, 0.75)
    communities_bt_df.append({"community": community,"category": category})
communities_bt_df = pd.DataFrame(communities_bt_df, columns=['community', 'category'])

```

```{python}
communities_bt_df.loc[communities_bt_df['category'] == 'videogames']
```

```{python}
print(getTitlesFromIDs(communities_bt_df.iloc[4]['community']).to_string())
print("___________")
print(getTitlesFromIDs(communities_bt_df.iloc[10]['community']).to_string())
print("___________")
print(getTitlesFromIDs(communities_bt_df.iloc[33]['community']).to_string())

```

### Most common words (title) for each community
Important where category==None

```{python}

```

## Plot

```{python}
def set_node_community(G, communities):
        '''Add community to node attributes'''
        for c, v_c in enumerate(communities):
            for v in v_c:
                # Add 1 to save 0 for external edges
                G.nodes[v]['community'] = c + 1
                
def get_color(i, r_off=1, g_off=1, b_off=1):
        '''Assign a color to a vertex.'''
        r0, g0, b0 = 0, 0, 0
        n = 16
        low, high = 0.1, 0.9
        span = high - low
        r = low + span * (((i + r_off) * 3) % n) / (n - 1)
        g = low + span * (((i + g_off) * 5) % n) / (n - 1)
        b = low + span * (((i + b_off) * 7) % n) / (n - 1)
        return (r, g, b)

def getNodeColors(graph):
    return [get_color(graph.nodes[v]['community']) for v in graph.nodes]

def readPos(fileName):
    in_file = open(fileName,"rb")
    text = in_file.read()
    in_file.close()
    return pickle.loads(text)

def writePos(pos, fileName):
    out_file = open(fileName,"wb")
    out_file.write(pickle.dumps(pos))
    out_file.close()

def plotNetworkWithExistingPos(graph, fileName, node_size=300, figsize=(100,100)):
    pos = readPos(fileName)
    plt.figure(figsize=figsize)
    node_color = getNodeColors(graph)
    nx.draw_networkx(graph, pos=pos, with_labels = False, node_color=node_color, \
                     edge_color="black", node_size=node_size)

def plotNetworkNew(graph, fileName):
    pos = nx.spring_layout(graph, k=0.1)
    writePos(pos, fileName)
    plt.figure(figsize=(100,100))
    node_color = getNodeColors(graph)
    # vedere se aggiungere nodelist=nodes[category==...] per fare solo categories piu importanti
    nx.draw_networkx(graph, pos=pos, with_labels = False, node_color=node_color)
```

```{python}
set_node_community(G_bt, communities_bt)
plotNetworkWithExistingPos(G_bt, 'pos.txt', figsize=(50,50))
```

```{python}
def filterone(G, clist):
    res = [v for v in G_bt.nodes if G_bt.nodes[v]["community"] in clist]
    return res
```

```{python}
filtered = filterone(G_bt, [1,2,3])
len(filtered)
```

```{python}
G_filtered=nx.Graph()
```

```{python}
G_filtered.add_nodes_from(filtered)
G_filtered.nodes
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1])), 'pos.txt', node_size=80, figsize=(4,4))
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1,2])), 'pos.txt', node_size=80, figsize=(4,4))
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1,2,3])), 'pos.txt', node_size=80, figsize=(10,10))
```

```{python}
# plot partitions generated with louvein

# color the nodes according to their partition
#plt.figure(figsize=(50,50))
#cmap = cm.get_cmap('viridis', max(partition.values()) + 1)
#nx.draw_networkx_nodes(G_bt, pos, partition.keys(), node_size=40,
                       cmap=cmap, node_color=list(partition.values()))
#nx.draw_networkx_edges(G_bt, pos, alpha=0.5)
#plt.show()
```

```{python}
# plotNetworkWithExistingPos(G_bt.subgraph(partition), 'pos.txt', node_size=80, figsize=(50,50))
```

## webApp functions

```{python}
data = pd.read_csv('../networkData/cytoProducts.csv', sep='\t')
data
```

```{python}
data = pd.read_json('../data/reviews.json', lines=True)
data
```

```{python}
cleanReviews(data)
```

```{python}
data['rating']
```

```{python}
plt.figure(figsize=(18,6))
data['rating'].plot(kind='hist')
```

```{python}
verified = data.loc[data['verified'] == True]
```

```{python}
helpful = verified.loc[verified['helpful'] > 0]
```

## Utils

```{python}
def generateProductsSampleCsv():
    df = pd.DataFrame(columns=data.columns)
    for index in range(1,11):
        df.loc[index] = data.iloc[randint(1,10000)]
    df.to_csv('../dataApp/sampleProducts.csv', columns=["_id", "title", "category", "price", "avg_rating", "pictures", "questions_number", "reviews_number"], sep='\t', index=False)

def generateCsvRatings():
    dic = {}
    for index in range(0,6):
        if index == 0:
            dic[index] = len(data.loc[data['avg_rating'] == index])
        elif index < 5:
            dic[index] = len(data.loc[(data['avg_rating'] > index) & (data['avg_rating'] < index+1)])
        else:
            dic[index] = len(data.loc[data['avg_rating'] == index])
    pd.DataFrame(list(dic.items()), columns=['rating', 'value']).to_csv('../dataApp/ratingsDistrib.csv', sep='\t', index=False)

def generateReviewsSampleCsv():
    data.head(10).to_csv('../dataApp/sampleReviews.csv', index=False, sep='\t')

def generateReviewsSampleTextCsv():
    data.loc[data['product'] == 'B00ESBHG3Q'].to_csv('../dataApp/sampleReviewsText.csv', index=False, sep='\t')

def generateRatingDistribCsv():
    distrib = helpful['rating'].value_counts()
    pd.DataFrame({'rating': distrib.keys(), 'value': distrib.values}).to_csv('../dataApp/ratingDistribFilteredReviews.csv', sep='\t', index=False)
    
def cleanReviews(data):
    data['rating'] = data['rating'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
    data['helpful'] = data['helpful'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
    
```

```{python}
generateCsvRatings()
```

```{python}
generateProductsSampleCsv()
```

```{python}
generateReviewsSampleCsv()
```

```{python}
generateReviewsSampleTextCsv()
```

```{python}
generateRatingDistribCsv()
```

```{python}
nx.number_connected_components(full_graph)
```

```{python}
def readGraph(network, i):
    with open ('../networkData/' + network, encoding='cp850') as prova:
        json_prova = json.load(prova)
        nodes = json_prova['elements']['nodes']
        edges = json_prova['elements']['edges']
    addFieldsToNodes(nodes, i)
    net = addEdges(nodes, edges)    
    return net
```

```{python}
def writeGraph(network, fileName):
    with open('../networkData/'+ fileName, 'w') as outfile:
        json.dump(network, outfile)
```

```{python}
def addEdges(nodes, edges):
        return nodes + edges
```

```{python}
def addFieldsToNodes(nodes, i):
    for element in nodes:
        elementId = element['data']['name']
        category = element['data']['category']
        community = element['data']['community']
        element['data']['catColor'] = categoriesDf.loc[categoriesDf['category'] == category]['color'].values[0]
        if community > 10:
            element['data']['commColor'] = '#999999'
        else:
            element['data']['commColor'] = communitiesDf.loc[communitiesDf['community'] == community]['color'].values[0]
        element['data']['nodeSize'] = int(element['data']['Degree']) * 10
        if elementId == communities_stats.iloc[i]['max_degree_id']:
            element['data']['centralNode'] = 1
        else:
            element['data']['centralNode'] = 0
```

```{python}
categoriesDf = pd.DataFrame({'category':categories.keys(), 'color': None})
random.seed(10)
for index, row in categoriesDf.iterrows():
    row['color'] = "#{:06x}".format(random.randint(0, 0xFFFFFF))
categoriesDf
```

```{python}
comm = communities_stats.head(10)
communitiesDf = pd.DataFrame({'community':comm['id'].values, 'color': None})
random.seed(50)
for index, row in communitiesDf.iterrows():
    row['color'] = "#{:06x}".format(random.randint(0, 0xFFFFFF))
communitiesDf
```

```{python}
net = readGraph('giantComponent.cyjs', 0)
```

```{python}
writeGraph(net, 'giantComponent.cyjs')
```

```{python}
for i in range(0,11):
    net = readGraph('community' + str(i+1) +'.cyjs', i)
    writeGraph(net, 'community'+str(i+1)+'.cyjs')
```

```{python}
# !pip install dandelion-eu
```

```{python}
pip install spacy-dbpedia-spotlight
```

```{python}
str = " ".join([x for (x,v) in communities_stats.iloc[0]["top_ents"].items()][0:3])
str
```

```{python}
communities_stats.iloc[0]
```

```{python}
from dandelion import DataTXT
dandelion_token = ''
datatxt = DataTXT(app_id="08244596bdc443adad8240b9064f5f86", app_key="08244596bdc443adad8240b9064f5f86")
response = datatxt.nex(str)
for annotation in response.annotations:
    print(annotation)
```

```{python}
response

```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
import itertools
from collections import Counter
import nltk
from nltk.stem.snowball import SnowballStemmer

import string
from nltk import wordpunct_tokenize

from wordcloud import WordCloud
from datetime import datetime
import pickle
import re

#progress bar
from tqdm import tqdm, tqdm_notebook

# instantiate
tqdm.pandas(tqdm_notebook)

# model evaluation
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate

#LDA
import gensim
import gensim.corpora as corpora
from gensim.models import CoherenceModel
import pyLDAvis.gensim
import pickle 
import pyLDAvis
```

### Scommentare se si usano i dati iniziali

```{python}
dataReviewsChunk = pd.read_json('../data/reviews.json', lines=True)
```

```{python}
dataReviewsChunk
```

```{python}

chunk_list = []  # append each chunk df here 

# Each chunk is in df format
for chunk in dataReviewsChunk:

    chunk_list.append(chunk)
```

```{python}
dataReviews = pd.concat(chunk_list)
```

```{python}
## elimino recensioni non verificate e quelli con helpfull == 0
```

```{python}
def convert_to_int(field):
    return field["$numberInt"]
```

```{python}
dataReviews["rating"]=dataReviews["rating"].apply(convert_to_int).astype(int)
```

```{python}
dataReviews["helpful"]=dataReviews["helpful"].apply(convert_to_int).astype(int)
```

```{python}
def getDataReviewsReduced():
    try:
        dataReviewsReduced = pd.read_csv("../data/dati_ridotti.csv", sep=",", index_col=0)
    except FileNotFoundError:
        dataReviewsReduced = dataReviews.loc[dataReviews["verified"] == True]
        dataReviewsReduced = dataReviewsReduced.loc[dataReviews["helpful"] != 0]
        dataReviewsReduced.to_csv("../data/dati_ridotti.csv", sep=",")
    return dataReviewsReduced
dataReviewsReduced = getDataReviewsReduced()
```

### Dati filtrati (vedi sopra)

```{python}
dataReviewsReduced
```

```{python}
counts = dataReviewsReduced["rating"].value_counts()
```

```{python}
counts.values
```

```{python}
x = counts._index
print(x)
y = counts.values
print(y)
```

```{python}
fig = plt.figure(figsize=(18,6))
sns.barplot(x=counts._index, y=counts.values)
plt.title("Rating distribution")
plt.show()
```

```{python}
print("Proportion of review with score=1: {}%".format(len(dataReviewsReduced[dataReviewsReduced.rating == 1]) / len(dataReviewsReduced)*100))
print("Proportion of review with score=2: {}%".format(len(dataReviewsReduced[dataReviewsReduced.rating == 2]) / len(dataReviewsReduced)*100))
print("Proportion of review with score=3: {}%".format(len(dataReviewsReduced[dataReviewsReduced.rating == 3]) / len(dataReviewsReduced)*100))
print("Proportion of review with score=4: {}%".format(len(dataReviewsReduced[dataReviewsReduced.rating == 4]) / len(dataReviewsReduced)*100))
print("Proportion of review with score=5: {}%".format(len(dataReviewsReduced[dataReviewsReduced.rating == 5]) / len(dataReviewsReduced)*100))
```

```{python}
# recensioni vuote
dataReviewsReduced[dataReviewsReduced["body"].str.len() == 0]
```

```{python}
dataReviewsReduced.loc[dataReviewsReduced['rating'] == 3 , 'polarity'] = 'neutral'
dataReviewsReduced.loc[dataReviewsReduced['rating'] > 3 , 'polarity'] = 'positive'
dataReviewsReduced.loc[dataReviewsReduced['rating'] < 3 , 'polarity'] = 'negative'
```

```{python}
counts = dataReviewsReduced["polarity"].value_counts()
fig = plt.figure(figsize=(18,6))
sns.barplot(x=counts._index, y=counts.values)
plt.title("Rating distribution")
plt.show()
```

```{python}
def undersampling(df):
    positive, negative, _ = df.polarity.value_counts()
    df_positive = df[df.polarity == 'positive']
    df_positive = df_positive.sample(negative, random_state=1)
    df_negative = df[df.polarity == 'negative']
    df = pd.concat([df_positive, df_negative])
    #df = df.sample(frac=1)
    return df
```

```{python}
new_dataReviews = undersampling(dataReviewsReduced)
```

```{python}
counts = new_dataReviews["polarity"].value_counts()
fig = plt.figure(figsize=(18,6))
sns.barplot(x=counts._index, y=counts.values)
plt.title("Rating distribution")
plt.show()
```

### Funzioni varie

```{python}
def flat_list(l):
    return  [item for sublist in l for item in sublist]
```

```{python}
def plot_common_tokens(tokens, title, n=20):
    sentences = (list(itertools.chain(tokens)))
    flat_sentences = flat_list(sentences)
    counts = Counter(flat_sentences)
    #print(counts.most_common(30))
    common_words = [word[0] for word in counts.most_common(n)]
    common_counts = [word[1] for word in counts.most_common(n)]
    fig = plt.figure(figsize=(18,6))
    sns.barplot(x=common_words, y=common_counts)
    plt.title(title)
    plt.show()
```

```{python}
def word_Cloud(sentences):
    flat_sentences = flat_list(sentences)
    counter = Counter(flat_sentences)
    cdict = dict(counter.most_common(50))

    wordcloud = WordCloud(background_color="white").generate_from_frequencies(cdict)
    plt.figure(figsize = (10, 8), facecolor = None) 
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()
```

### Tokenizzazione

```{python}
new_dataReviews['token']=new_dataReviews['body'].progress_apply(word_tokenize)
```

```{python}
#new_dataReviews['token'] #Name: token, Length: 130648, dtype: object
```

### Stopwords

```{python}
stop_words=nltk.corpus.stopwords.words('italian')
stop_words
```

```{python}
new_dataReviews["cleaned"] = new_dataReviews["token"].progress_apply(lambda sentence : [word for word in sentence if word.lower() not in stop_words])
```

### Punctuation

```{python}
punctuation = string.punctuation
punctuation = punctuation + "..."+ "''" + "``" + "--"
print(punctuation)
```

```{python}
new_dataReviews["cleaned"] = new_dataReviews["cleaned"].progress_apply(lambda sentence : [word for word in sentence if word not in punctuation])
```

### Numbers

```{python}
regex_numbers = r'(?:(?:\d+,?)+(?:\.?\d+)?)'
```

```{python}
new_dataReviews["cleaned"] = new_dataReviews["cleaned"].progress_apply(lambda sentence : [re.sub(regex_numbers,"",word) for word in sentence if re.sub(regex_numbers,"",word) != ""])
```

### Eliminazione token di lunghezza 1

```{python}
new_dataReviews["cleaned"] = new_dataReviews["cleaned"].progress_apply(lambda sentence : [word for word in sentence if len(word)> 1])
```

```{python}
# tutto il processing si può fare in un unico loop TODO
```

```{python}
#new_dataReviews.to_csv("dati_puliti.csv")
```

```{python}
plot_common_tokens(new_dataReviews['cleaned'],'Most Common Tokens used in Reviews')
```

```{python}
len(new_dataReviews["cleaned"])
```

```{python}
word_Cloud(new_dataReviews["cleaned"])
```

```{python}
sentences = (list(itertools.chain(new_dataReviews["cleaned"])))
flat_sentences = flat_list(sentences)
counts = Counter(flat_sentences)
counts.most_common()
```

### Stemming

```{python}
stemmer = SnowballStemmer("italian")
def stemming_token(sentence,stemmer):
    stem = []
    for elem in sentence:
        stem.append(stemmer.stem(elem))
    return stem
```

```{python}
new_dataReviews["stemming"] = new_dataReviews["cleaned"].progress_apply(lambda x: stemming_token(x, stemmer))
```

```{python}
#new_dataReviews = pd.read_csv("../data/dati_finali.csv", sep=",", index_col=0)
```

```{python}
### da fare solo se si legge il csv finale
```

```{python}
#def str_to_list(sentence):
#   return ast.literal_eval(sentence)
```

```{python}
#import ast

#new_dataReviews["stemming"] = new_dataReviews["stemming"].progress_apply(str_to_list)
```

```{python}
new_dataReviews
```

```{python}
len(new_dataReviews)
```

### CountVectorizer

```{python}
try:
    with open('../model/bow.bin', 'rb') as f:
        # The protocol version used is detected automatically, so we do not
        # have to specify it.
        bow = pickle.load(f)
except FileNotFoundError:
    count_vect = CountVectorizer(stop_words=None, lowercase=True)
    #lowercase = true -> Convert all characters to lowercase before tokenizing.
    #stop_words = None -> If None, no stop words will be used
    bow = count_vect.fit(new_dataReviews['stemming'].apply(lambda x: " ".join(x)))
    
    with open('../model/bow.bin', 'wb') as f:
        pickle.dump(bow, f, pickle.HIGHEST_PROTOCOL)
        #s = pickle.dumps(model)
```

```{python}
count_vect
```

```{python}
bow.get_feature_names()[::2000]
```

```{python}
#X_train, X_test, y_train, y_test = train_test_split(bow, new_dataReviews['polarity'], test_size=0.2, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(new_dataReviews['stemming'].apply(lambda x: " ".join(x)), new_dataReviews['polarity'], test_size=0.2, random_state=1)    
```

```{python}
print("train size: ",len(X_train))
print("test size:",len(X_test))
```

```{python}
print("y train distribution:\n",y_train.value_counts())
print("y train distribution:\n",y_test.value_counts())
```

```{python}
def train_and_evaluate_model():
    model = LogisticRegression()
    model.fit(bow.transform(X_train), y_train)

    predictions = model.predict(bow.transform(X_test))
    print("predictions:\n{}".format(predictions))

    my_accuracy_score = accuracy_score(y_test, predictions)
    print("accuracy_score:\n{}".format(my_accuracy_score))

    cmatrix = confusion_matrix(y_test, predictions)
    print("confusion matrix:\n{}".format(cmatrix))

    scores = cross_val_score(model, bow.transform(new_dataReviews['stemming'].apply(lambda x: " ".join(x))), new_dataReviews['polarity'], cv=10)
    print("cross_val_score:\n{}".format(scores))
    print("Accuracy cross_val_scores: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

    scoring = ['precision_macro', 'recall_macro']
    scores = cross_validate(model, bow.transform(new_dataReviews['stemming'].apply(lambda x: " ".join(x))), new_dataReviews['polarity'], scoring=scoring,
                            cv=10, return_train_score=False)
    print("cross_validate with scoring = {}:\n{}".format(scoring, scores))
    print("Accuracy cross_validate with scoring ...: %0.2f (+/- %0.2f)" % (scores['test_precision_macro'].mean(), scores['test_precision_macro'].std() * 2)) 
```

### Use model with costum sentences

```{python}
def clean_sentence(sentence):
    tokens = word_tokenize(sentence)
    tokens_clean = []
    for word in tokens:
        if word.lower() not in stop_words and word.lower() not in punctuation and not word.isnumeric() and len(word)> 1:
            
            tokens_clean.append(stemmer.stem(word))
    return ' '.join(tokens_clean)
```

```{python}
sentence="belle le scarpe e le stringhe per il colore , però sono comode"
```

```{python}
clean_sentence(sentence)
```

```{python}
# codice per caricare il modello per convertire frasi in array
try:
    with open('../model/bow.bin', 'rb') as f:
        # The protocol version used is detected automatically, so we do not
        # have to specify it.
        bow = pickle.load(f)
except FileNotFoundError:
    # to save model in file.bin
    train_and_evaluate_model()
    with open('../model/model2.bin', 'wb') as f:
        pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)
```

```{python}
print(bow.transform([clean_sentence(sentence)]))
```

```{python}
model.predict(bow.transform([clean_sentence(sentence)]))
```

### LDA

```{python}
#https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0
```

```{python}
prod = "B0058BXHWE" 
new_dataReviews_prodotto = new_dataReviews[new_dataReviews["product"]  == prod]
```

### unigrammi

```{python}
texts = new_dataReviews_prodotto['cleaned']
```

```{python}
# creo il dizionario
dictionary = corpora.Dictionary(texts) 
```

```{python}
# Term Document Frequency -> converto testo in array
corpus = [dictionary.doc2bow(text) for text in texts]
#corpus
```

```{python}
def compute_coherence_values(corpus, dictionary, num_topics, texts):
    
    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=num_topics, 
                                           random_state=1)
    
    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
    
    return coherence_model_lda.get_coherence()
```

```{python}
def optimal_topics(dictionary, corpus, texts, max_topic):
    count=1
    c_v = []
    for num_topics in range(1, max_topic):
        print(count)
        coherence =  compute_coherence_values(corpus, dictionary, num_topics, texts)
        
        c_v.append(coherence)
        count += 1
    x = range(1, max_topic)
    plt.plot(x, c_v)
    plt.xlabel("num_topics")
    plt.ylabel("Coherence score")
    plt.legend(("c_v"), loc='best')
    plt.show()

    return c_v
```

```{python}
num_topics = optimal_topics(dictionary=dictionary, corpus=corpus, texts=texts, max_topic=10)
```

```{python}
num_topics
```

```{python}
ntopics=num_topics.index(max(num_topics))+1
```

```{python}
ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=ntopics, id2word=dictionary)
```

```{python}
#ldamodel.save('ldamodel1.gensim')
```

```{python}
topics = ldamodel.print_topics()
```

```{python}
topics
```

```{python}
x = ldamodel.show_topics(num_topics=ntopics, num_words=15, formatted=False)
topics_words = [[wd[0] for wd in tp[1]] for tp in x]
```

```{python}
for sentence in topics_words:
    print(" ".join(sentence))
```

```{python}
# Visualize the topics
pyLDAvis.enable_notebook()
LDAvis_prepared = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
LDAvis_prepared
```

#### LDA su bigrammi

```{python}
text_data = []
for sentence in new_dataReviews_prodotto['cleaned']:
    bigram = list(nltk.bigrams(sentence))
    tokens = []
    for i in bigram:
        tokens.append((''.join([w + ' ' for w in i])).strip())
    text_data.append(tokens)
```

```{python}
text_data
```

```{python}
texts=text_data
dictionary = corpora.Dictionary(texts) 
corpus = [dictionary.doc2bow(text) for text in texts]
```

```{python}
num_topics = optimal_topics(dictionary=dictionary, corpus=corpus, texts=texts, max_topic=10)
```

```{python}
ntopics=num_topics.index(max(num_topics))+1
ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=ntopics, id2word=dictionary)
```

```{python}
topics = ldamodel.print_topics()
x = ldamodel.show_topics(num_topics=ntopics, num_words=15, formatted=False)
topics_words = [[wd[0] for wd in tp[1]] for tp in x]
for sentence in topics_words:
    print(sentence)
```

```{python}
ntopics
```

```{python}
pyLDAvis.enable_notebook()
LDAvis_prepared = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
LDAvis_prepared
# mi ha dato eccezione
```

### Sentimenti recensioni neutre

```{python}
#dataReviews_neutre = pd.read_csv("../data/dati_ridotti.csv", sep=",", index_col=0)
dataReviews_neutre = dataReviewsReduced if dataReviewsReduced is not None else getDataReviewsReduced()
```

```{python}
dataReviews_neutre = dataReviews_neutre.loc[dataReviews_neutre["rating"]==3]
```

```{python}
dataReviews_neutre
```

```{python}
dataReviews_neutre["polarity"] = dataReviews_neutre["body"].progress_apply(lambda sentence: model.predict(bow.transform([clean_sentence(sentence)])))
```

```{python}
dataReviews_neutre["polarity"]
```

```{python}
print("Proportion of review with score=3 that is positive: {}%".format(len(dataReviews_neutre[dataReviews_neutre.polarity == "positive"]) / len(dataReviews_neutre)*100))
print("Proportion of review with score=3 that is negative: {}%".format(len(dataReviews_neutre[dataReviews_neutre.polarity == "negative"]) / len(dataReviews_neutre)*100))
```

```{python}
dataReviews_neutre["polarity"].value_counts().plot(kind="bar")

```

### Prodotti nel tempo

```{python}
#dataReviewsReduced = pd.read_csv("../data/dati_ridotti.csv", sep=",", index_col=0)
dataReviews_neutre = dataReviewsReduced if dataReviewsReduced is not None else getDataReviewsReduced()
```

```{python}
# look for good examples
top_prods = dataReviews.groupby('product').count().sort_values('_id', ascending=False).head(4)
```

```{python}
"B01ETRGE7M" in top_prods.index
```

```{python}
# GOOD EXAMPLES:
# B01ETRGE7M bello
# B00LPHUTOO altalena
# B01EWQ10D8 non male
# B0058BXHWE resegono
# ... non ne ho piu provati di quelli del box sopra
prod = "B01ETRGE7M" 
dataReviews_prodotti = dataReviews[dataReviews["product"].isin(top_prods.index)]
```

```{python}
dataReviews_prodotti
```

```{python}
dataReviews_prodotti["Period"] = dataReviews_prodotti["date"].apply(lambda x: x.strftime('%Y-%m'))
```

```{python}
dataReviews_prodotti["Period"].iloc[0]
```

```{python}
dataReviews_prodotti
```

```{python}
dataReviews_prodotti["polarity"] = dataReviews_prodotti["body"].progress_apply(lambda sentence: model.predict(bow.transform([clean_sentence(sentence)]))[0])
```

```{python}
dataReviews_prodotti
```

```{python}
dataReviews_prodotti["polarityNum"] = dataReviews_prodotti["polarity"].apply(lambda x: 1 if x == "positive" else 0)
```

```{python}
dataReviews_prodotti
```

```{python}
#dataReviews_prodotti_month = dataReviews_prodotti.groupby(['product', 'Period']).progress_apply(lambda x: len(x.loc[x["polarity"]=="positive"])/len(x)).to_frame("polarity")

# use the rating intead of polarity
dataReviews_prodotti_month = dataReviews_prodotti[["product", "Period", "rating", "polarityNum"]]\
    .groupby(['product', 'Period']).mean()#.progress_apply(lambda x: len(x.loc[x["rating"]>3])/len(x))



```

```{python}
def trendline(df, order=1):
    coeffs = np.polyfit(range(0,len(df.index)), df, order)
    #slope = coeffs[-2]
    return coeffs[-2], [coeffs[-2] * x + coeffs[-1] for x in range(0,len(df.index))]


```

```{python}
dataReviews_prodotti_month["rating"] = dataReviews_prodotti_month["rating"] / 5
```

```{python}
dataReviews_prodotti_month.loc["B00G9WHN12"].plot()
```

```{python}
dataReviews_prodotti_month[["rating", "polarityNum"]]
```

```{python}
coeffs = []
lines = []
for i in dataReviews_prodotti_month.index.get_level_values(0).value_counts().index:
    coeff, line = trendline(dataReviews_prodotti_month.loc[i]["polarityNum"])
    lines += line
    coeffs += [coeff] * len(line)
```

```{python}
len(lines)
```

```{python}
dataReviews_prodotti_month["trendCoeff"] = coeffs
dataReviews_prodotti_month["trendLine"] = lines
```

```{python}
dataReviews_prodotti_month.loc["B00G9WHN12"].plot()
```

```{python}
dataReviews_prodotti_month.to_csv("../data/datarevies_prodotti_month.csv")
```
