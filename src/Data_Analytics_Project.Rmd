---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

## Imports

```{python}
# !pip install --upgrade -r requirements.txt
```

```{python}
import pandas as pd
import nltk
import operator
import nltk
import scipy

from collections import Counter
# #%matplotlib notebook
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import numpy as np

import pickle
# import igraph as ig
import networkx as nx
from community import generate_dendrogram, community_louvain
from networkx.algorithms.community import greedy_modularity_communities
```

## Data importation

```{python}
data = pd.read_json('../data/products.json', lines=True)
data
```

```{python}
# clean numeric values
def cleanProducts():
    data['price'] = data['price'].apply(lambda dictValue : float(list(dictValue.values())[0]) if dictValue != None else float(0))
    data['avg_rating'] = data['avg_rating'].apply(lambda dictValue : float(list(dictValue.values())[0]) if dictValue != None else float(0))
    data['reviews_number'] = data['reviews_number'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
    data['questions_number'] = data['questions_number'].apply(lambda dictValue : int(list(dictValue.values())[0]) if dictValue != None else int(0))
```

```{python}
cleanProducts()
data.head(3)
```

# Network


## Utils

```{python}
def filterEdges(edges, weight):
    return edges.loc[edges['weight'] in weight]

def csvCreateNodes():
    # export nodes for cytoscape
    data['title'] = data['title'].replace(to_replace='[ \t]+', value=' ', regex=True)
    data.to_csv("../networkData/cytoProducts.csv",columns=["_id", "title", "category", "price", "avg_rating", "reviews_number", "questions_number", 'community'], sep="\t", index=False)

def csvCreateEdges():        
    # create complete list of edges
    edges = []
    for i, row in data.iterrows():
        for subrow in row['bought_together']:
            ft = {'from': row['_id'], 'to': subrow, 'weight': 3}
            edges.append(ft)
        for subrow in row['also_bought']:
            ft = {'from': row['_id'], 'to': subrow, 'weight': 2}
            edges.append(ft)
        for subrow in row['also_viewed']:
            ft = {'from': row['_id'], 'to': subrow, 'weight': 1}
            edges.append(ft)

    edges = pd.DataFrame(edges)
    # keep a copy the complete list... maybe it will be useful in future
    all_edges = pd.DataFrame(edges)

    nodes = data['_id']
    print("# edges:", len(edges))
    print("# nodes:", len(nodes))
    
    # remove edges with not available products
    for i, edge in edges.iterrows():
        if edge['to'] in data['_id'].values:
            continue
        else:
            edges = edges.drop(i)

    print("# edges:", len(edges))
    
    # remove also_viewed and merge also_bought with bought_together
    edges = all_edges.loc[all_edges['weight'] != 1][['from','to']]
    print("# edges:", len(edges))
    edges
    
    # remove symmetries
    edges = pd.DataFrame.from_records(list({tuple(sorted(item)) for item in edges.values}), columns=['from', 'to'])
    edges = edges.drop_duplicates(subset=["from", "to"])

    print("# edges:", len(edges))
    
    # export edges for cytoscape
    edges.to_csv("../networkData/cytoEdges.csv", sep="\t", columns=["from", "to"], index=False)
```

# Network analysis


## Read/Create edges

```{python}
for i in range(1,3):
    try:
        edges = pd.read_csv('../networkData/cytoEdges.csv', sep='\t')
        break
    except:
        csvCreateEdges()
edges
```

## Create full network

```{python}
full_graph = nx.from_pandas_edgelist(edges, source='from', target='to')
```

## Read/Create nodes

```{python}
data = pd.DataFrame(data.loc[data['_id'].isin(list(full_graph))])
data
```

## Filter products by categories

```{python}
categories = data['category'].value_counts()
print("number of categories: {}".format(len(categories)))
print(categories)
categories_dict = categories.to_dict()
categories_lbl = list(categories.keys())
```

## Example: centralities of a category

```{python}
products_by_cat = data.loc[data['category'] == 'musical-instruments']['_id'].values
subgraph_cat = full_graph.subgraph(products_by_cat)
degree_centrality = nx.degree_centrality(subgraph_cat)
degree_centrality = dict(sorted(degree_centrality.items(), key=operator.itemgetter(1),reverse=True))
degrees_dict = dict(subgraph_cat.degree)
degrees_dict = dict(sorted(degrees_dict.items(), key=operator.itemgetter(1),reverse=True))
betweenness_centrality = nx.betweenness_centrality(subgraph_cat)
betweenness_centrality = dict(sorted(betweenness_centrality.items(), key=operator.itemgetter(1),reverse=True))
closeness_centrality = nx.closeness_centrality(subgraph_cat)
closeness_centrality = dict(sorted(closeness_centrality.items(), key=operator.itemgetter(1),reverse=True))
eigenvector_centrality = nx.eigenvector_centrality(subgraph_cat, max_iter=1000)
eigenvector_centrality = dict(sorted(eigenvector_centrality.items(), key=operator.itemgetter(1),reverse=True))
```

## Compute stats for each category

```{python}
def computeStats(data, graph):
    subgraph = graph.subgraph(data)
    # cardinality
    cardinality = len(data)
    # degree centrality
    degree_centrality = nx.degree_centrality(subgraph)
    degree_centrality = dict(sorted(degree_centrality.items(), key=operator.itemgetter(1),reverse=True))
    max_degree_id = list(degree_centrality.keys())[0]
    # degree
    degrees = dict(subgraph.degree)
    degrees = dict(sorted(degrees.items(), key=operator.itemgetter(1),reverse=True))
    max_degree = list(degrees.values())[0]
    # TODO (maybe): most positive/negative products by sentiment
    # TODO (maybe): reviews number
    return cardinality, max_degree_id, max_degree

          
```

```{python}
categories_stats = pd.DataFrame(columns = ['category','cardinality','max_degree_id', 'max_degree', 'most_positive_id'])

for category in categories_lbl:
    data_by_category = data.loc[data['category'] == category]['_id'].values
    cardinality, max_degree_id, max_degree = computeStats(data_by_category, full_graph)
    categories_stats = categories_stats.append({'category':category,'cardinality':cardinality, 'max_degree_id':max_degree_id,\
                             'max_degree':max_degree}, ignore_index = True)
    

```

```{python}
categories_stats
```

## Communities detection


### Utils

```{python}
# communities detection
def getCommunities(graph):
    return sorted(greedy_modularity_communities(graph), key=len, reverse=True)
    
def getCommunitiesLouvein(network):
    return community_louvain.best_partition(network)

def getCategoryStatZ(categories, threshold, top):
    z = scipy.stats.zscore(categories)
    max_cat = categories.keys()[z.argmax()] if z.max() > threshold else None
    distrib = categories[z.argsort()[::-1]].head(top)
    others_value = sum(categories.iloc[top:len(categories)])
    # z[::-1].sort()
    # distrib = dict((ki,zi) for ki,zi in zip(distrib.keys(), z))
    distrib = dict((ki,zi) for ki,zi in zip(distrib.keys(), distrib.values))
    distrib['others'] = others_value
    return max_cat, distrib

def getCategoryStats(categories, threshold, top):  
    categories = categories.sort_values(ascending=False)
    tot = sum(categories)
    max_val = threshold*tot
    max_cat = None
    #distrib = pd.DataFrame(columns = ['category','value'])
    distrib = {}
    for key, value in categories.items():
        # dominant category
        if value >= max_val:
            max_val = value
            max_cat = key
        # categories distribution
        if key in categories.head(top):
            #distrib = distrib.append({'category':key, 'value': value}, ignore_index = True)
            distrib[key] = value
    
    # categories distribution
    others_value = sum(categories.iloc[top:len(categories)])
    #distrib.append({'category': 'others', 'value': others_value}, ignore_index = True)
    distrib['others'] = others_value
    return max_cat, distrib

def getTitlesFromIDs(ids):
    return data.loc[data['_id'].isin(ids)]['title']
```

### Communities analysis

```{python}
communities = getCommunities(full_graph)
print("# communities:", len(communities))
```

```{python}
communities = [list(x) for x in communities]
```

```{python}
communities_stats = pd.DataFrame(columns = ['id','community', 'dominant_category', 'cardinality', 'max_degree_id', 'max_degree', 'top_words'])
for i, community in enumerate(communities):
    # dominant category
    categories = data.loc[data['_id'].isin(community)].groupby('category').count()['_id']
    #dominant_category, categories_distribution = getCategoryStats(categories, 0.75, 3)
    dominant_category, categories_distribution = getCategoryStatZ(categories, 1.5, 3)
    # compute stats
    cardinality, max_degree_id, max_degree = computeStats(community, full_graph)
    communities_stats = communities_stats.append({'id': i+1,'community': community,'dominant_category': dominant_category,\
                                                  'categories_distribution':categories_distribution,'cardinality':cardinality,\
                                                  'max_degree_id':max_degree_id, 'max_degree':max_degree}, ignore_index = True)
communities_stats
```

## Assign community to each product

```{python}
for _, community in communities_stats.iterrows():
    rows = list(data.index[data['_id'].isin(community['community'])])
    data.loc[rows, 'community'] = int(community['id'])
data
```

### Write cytoscape products csv

```{python}
csvCreateNodes()
```

## Most common words for each community

```{python}
nltk.download('punkt')
nltk.download('stopwords')
#nltk.download('averaged_perceptron_tagger')
#nltk.download('maxent_ne_chunker')
#nltk.download('words')
```

```{python}
#communities_stats['category'].count()
#communities_stats.loc[communities_stats['dominant_category'].isnull()].sort_values('cardinality', ascending=False)['categories_distribution'].values
#type(communities_stats.iloc[1])
#communities_stats
stop_words = nltk.corpus.stopwords.words('italian') + nltk.corpus.stopwords.words('english')

def processMergedTitles(merged_titles):
    ret = nltk.tokenize.word_tokenize(merged_titles)
    #ret = nltk.chunk.ne_chunk(nltk.pos_tag(ret))
    ret = [word for word in ret if len(word)>2 and word.lower() not in stop_words]
    ret = Counter(ret)
    return ret.most_common(10)

for i, row in communities_stats.iterrows():
    merged_titles = " ".join([data.loc[data['_id'] == product_id].iloc[0]["title"] for product_id in row["community"]])
    #print([product_id for product_id in row["community"]])
    row["top_words"]=processMergedTitles(merged_titles)
```

```{python}
communities_stats.iloc[10:20][["top_words", "dominant_category"]]
```

```{python}
communities_stats.loc[communities_stats["dominant_category"].isnull()]["categories_distribution"]
```

```{python}
prova = data.loc[data['_id'].isin(communities[0])].groupby('category').count()['_id']
top = 3

sum(prova.sort_values(ascending=False).head(top))+sum(prova.sort_values(ascending=False).iloc[top:len(prova)])

```

## Bridges

```{python}
bridges = list(nx.bridges(full_graph))
```

```{python}
bridges_cat = pd.DataFrame(columns=['id_from', 'id_to', 'cat_from', 'cat_to'])
for edge in bridges:
    cat_from = data.loc[data['_id'] == edge[0]]['category'].values[0]
    cat_to = data.loc[data['_id'] == edge[1]]['category'].values[0]
    if cat_from != cat_to:
        bridges_cat = bridges_cat.append({'id_from': edge[0], 'id_to': edge[1], 'cat_from': cat_from ,\
                                          'cat_to': cat_to}, ignore_index=True)
bridges_cat
```

```{python}
bridges_betweenness = nx.edge_betweenness_centrality(full_graph)
```

```{python}
bridges_cat = pd.DataFrame(columns=['id_from', 'id_to', 'cat_from', 'cat_to', 'edge_betweenness'])
for edge in bridges:
    cat_from = data.loc[data['_id'] == edge[0]]['category'].values[0]
    cat_to = data.loc[data['_id'] == edge[1]]['category'].values[0]
    if cat_from != cat_to:
        bridges_cat = bridges_cat.append({'id_from': edge[0], 'id_to': edge[1], 'cat_from': cat_from ,\
                                          'cat_to': cat_to, 'edge_betweenness': bridges_betweenness[edge]}, ignore_index=True)
bridges_cat
```

```{python}
bridges_cat.to_csv('./bridges.csv',sep="\t", index=False)
```

## ! read csv !

```{python}
bridges_cat = pd.read_csv('./bridges.csv', sep='\t')
```

```{python}
cat_in_bridges = bridges_cat[['cat_from','cat_to']].drop_duplicates().values
bridges_stats = pd.DataFrame()
for cat in cat_in_bridges:
    filtered_bridges = bridges_cat.loc[(bridges_cat['cat_from']==cat[0]) & (bridges_cat['cat_to']==cat[1])]
    bridges_stats = bridges_stats.append(filtered_bridges.loc[filtered_bridges['edge_betweenness'] == max(filtered_bridges['edge_betweenness'])].head(1), ignore_index=True)
bridges_stats

```

## Create a network for each relation (OLD)

```{python}
# network for bought_together
edges_bt = filterEdges(all_edges, 3)
G_bt = nx.from_pandas_edgelist(edges_bt, source='from', target='to')
print("### bought_together ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_bt))
print()
# network for also_bought
edges_ab = filterEdges(all_edges, 2)
G_ab = nx.from_pandas_edgelist(edges_ab, source='from', target='to')
print("### also_bought ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_ab))
print()

# network for also_viewed
edges_av = filterEdges(all_edges, 1)
G_av = nx.from_pandas_edgelist(edges_av, source='from', target='to')
print("### also_viewed ###")
print("# nodes:", len(nodes))
print("# edges:", len(edges_av))
print()


```

## Communities


### Single relations (OLD)

```{python}
communities_bt = getCommunities(G_bt)
print("# communities bt:", len(communities_bt))

communities_ab = getCommunities(G_ab)
print("# communities ab:", len(communities_ab))

communities_av = getCommunities(G_av)
print("# communities av:", len(communities_av))

```

```{python}
# get communities with louvein
partition = getCommunitiesLouvein(G_ab)
```

```{python}
print(len(partition))
partition_df = pd.DataFrame(partition.items(), columns=['_id', 'community'])

grouped_df = partition_df.groupby('community').count()
print(len(grouped_df))
grouped_df.sort_values('_id').loc[grouped_df['_id'] >= 5]
```

#### bt examples

```{python}
# products in a community

communities_bt_list =[list(x) for x in communities_bt]
community_bt_x = communities_bt_list[3]
community_bt_x
print(data.loc[data['_id'].isin(community_bt_x)].groupby('category').count())

```

```{python}
data.loc[data['_id'].isin(community_bt_x)]
```

#### also viewed examples

```{python}
communities_av_list =[list(x) for x in communities_av]

community_av_1 = communities_av_list[4]
community_av_1
print(data.loc[data['_id'].isin(community_av_1)].groupby('category').count())
data.loc[data['_id'].isin(community_av_1)]
```

#### predominant category for each community

```{python}
communities_bt = [list(x) for x in getCommunities(G_bt)]

```

```{python}
#print(communities_bt)
communities_bt_df = []
#pd.DataFrame(columns=["community", "category"])
for community in communities_bt:
    categories = data.loc[data['_id'].isin(community)].groupby('category').count()['_id']
    category = getDominantCategory(categories, 0.75)
    communities_bt_df.append({"community": community,"category": category})
communities_bt_df = pd.DataFrame(communities_bt_df, columns=['community', 'category'])

```

```{python}
communities_bt_df.loc[communities_bt_df['category'] == 'videogames']
```

```{python}
print(getTitlesFromIDs(communities_bt_df.iloc[4]['community']).to_string())
print("___________")
print(getTitlesFromIDs(communities_bt_df.iloc[10]['community']).to_string())
print("___________")
print(getTitlesFromIDs(communities_bt_df.iloc[33]['community']).to_string())

```

### Most common words (title) for each community
Important where category==None

```{python}

```

## Plot

```{python}
def set_node_community(G, communities):
        '''Add community to node attributes'''
        for c, v_c in enumerate(communities):
            for v in v_c:
                # Add 1 to save 0 for external edges
                G.nodes[v]['community'] = c + 1
                
def get_color(i, r_off=1, g_off=1, b_off=1):
        '''Assign a color to a vertex.'''
        r0, g0, b0 = 0, 0, 0
        n = 16
        low, high = 0.1, 0.9
        span = high - low
        r = low + span * (((i + r_off) * 3) % n) / (n - 1)
        g = low + span * (((i + g_off) * 5) % n) / (n - 1)
        b = low + span * (((i + b_off) * 7) % n) / (n - 1)
        return (r, g, b)

def getNodeColors(graph):
    return [get_color(graph.nodes[v]['community']) for v in graph.nodes]

def readPos(fileName):
    in_file = open(fileName,"rb")
    text = in_file.read()
    in_file.close()
    return pickle.loads(text)

def writePos(pos, fileName):
    out_file = open(fileName,"wb")
    out_file.write(pickle.dumps(pos))
    out_file.close()

def plotNetworkWithExistingPos(graph, fileName, node_size=300, figsize=(100,100)):
    pos = readPos(fileName)
    plt.figure(figsize=figsize)
    node_color = getNodeColors(graph)
    nx.draw_networkx(graph, pos=pos, with_labels = False, node_color=node_color, \
                     edge_color="black", node_size=node_size)

def plotNetworkNew(graph, fileName):
    pos = nx.spring_layout(graph, k=0.1)
    writePos(pos, fileName)
    plt.figure(figsize=(100,100))
    node_color = getNodeColors(graph)
    # vedere se aggiungere nodelist=nodes[category==...] per fare solo categories piu importanti
    nx.draw_networkx(graph, pos=pos, with_labels = False, node_color=node_color)
```

```{python}
set_node_community(G_bt, communities_bt)
plotNetworkWithExistingPos(G_bt, 'pos.txt', figsize=(50,50))
```

```{python}
def filterone(G, clist):
    res = [v for v in G_bt.nodes if G_bt.nodes[v]["community"] in clist]
    return res
```

```{python}
filtered = filterone(G_bt, [1,2,3])
len(filtered)
```

```{python}
G_filtered=nx.Graph()
```

```{python}
G_filtered.add_nodes_from(filtered)
G_filtered.nodes
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1])), 'pos.txt', node_size=80, figsize=(4,4))
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1,2])), 'pos.txt', node_size=80, figsize=(4,4))
```

```{python}
plotNetworkWithExistingPos(G_bt.subgraph(filterone(G_bt, [1,2,3])), 'pos.txt', node_size=80, figsize=(10,10))
```

```{python}
# plot partitions generated with louvein

# color the nodes according to their partition
#plt.figure(figsize=(50,50))
#cmap = cm.get_cmap('viridis', max(partition.values()) + 1)
#nx.draw_networkx_nodes(G_bt, pos, partition.keys(), node_size=40,
                       cmap=cmap, node_color=list(partition.values()))
#nx.draw_networkx_edges(G_bt, pos, alpha=0.5)
#plt.show()
```

```{python}
# plotNetworkWithExistingPos(G_bt.subgraph(partition), 'pos.txt', node_size=80, figsize=(50,50))
```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```
