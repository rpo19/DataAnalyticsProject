---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
import itertools
from collections import Counter
import nltk
from nltk.stem.snowball import SnowballStemmer

import string
from nltk import wordpunct_tokenize

#from wordcloud import WordCloud
from datetime import datetime
import pickle
import re

#progress bar
from tqdm import tqdm, tqdm_notebook

# instantiate
tqdm.pandas(tqdm_notebook)

# model evaluation
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate

#LDA
import gensim
import gensim.corpora as corpora
from gensim.models import CoherenceModel
import pyLDAvis.gensim
import pickle 
import pyLDAvis
```

### Scommentare se si usano i dati iniziali

```{python}
'''dataReviewsChunk = pd.read_json('../data/reviews.json', lines=True, chunksize=10000)'''
```

```{python}
'''dataReviews'''
```

```{python}
'''
chunk_list = []  # append each chunk df here 

# Each chunk is in df format
for chunk in dataReviewsChunk:

    chunk_list.append(chunk)
'''
```

```{python}
#dataReviews = pd.concat(chunk_list)
```

```{python}
## elimino recensioni non verificate e quelli con helpfull == 0
```

```{python}
'''def convert_to_int(field):
    return field["$numberInt"]'''
```

```{python}
'''dataReviews["rating"]=dataReviews["rating"].apply(convert_to_int).astype(int)'''
```

```{python}
'''dataReviews["helpful"]=dataReviews["helpful"].apply(convert_to_int).astype(int)'''
```

```{python}
'''dataReviews = dataReviews.loc[dataReviews["verified"] == True]'''
```

```{python}
'''len(dataReviews.loc[dataReviews["verified"] == False])'''
```

```{python}
'''len(dataReviews.loc[dataReviews["helpful"] == 0]) - len(dataReviews.loc[dataReviews["helpful"] != 0])'''
```

```{python}
'''dataReviews = dataReviews.loc[dataReviews["helpful"] != 0]
dataReviews'''
```

```{python}

```

### Dati filtrati (vedi sopra)

```{python}
dataReviews = pd.read_csv("../data/dati_ridotti.csv", sep=",", index_col=0)
```

```{python}
dataReviews
```

```{python}
counts = dataReviews["rating"].value_counts()
```

```{python}
counts.values
```

```{python}
x = counts._index
print(x)
y = counts.values
print(y)
```

```{python}
fig = plt.figure(figsize=(18,6))
sns.barplot(x=counts._index, y=counts.values)
plt.title("Rating distribution")
plt.show()
```

```{python}
print("Proportion of review with score=1: {}%".format(len(dataReviews[dataReviews.rating == 1]) / len(dataReviews)*100))
print("Proportion of review with score=2: {}%".format(len(dataReviews[dataReviews.rating == 2]) / len(dataReviews)*100))
print("Proportion of review with score=3: {}%".format(len(dataReviews[dataReviews.rating == 3]) / len(dataReviews)*100))
print("Proportion of review with score=4: {}%".format(len(dataReviews[dataReviews.rating == 4]) / len(dataReviews)*100))
print("Proportion of review with score=5: {}%".format(len(dataReviews[dataReviews.rating == 5]) / len(dataReviews)*100))
```

```{python}
# recensioni vuote
dataReviews[dataReviews["body"].str.len() == 0]
```

```{python}
dataReviews.loc[dataReviews['rating'] == 3 , 'polarity'] = 'neutral'
dataReviews.loc[dataReviews['rating'] > 3 , 'polarity'] = 'positive'
dataReviews.loc[dataReviews['rating'] < 3 , 'polarity'] = 'negative'
```

```{python}
counts = dataReviews["polarity"].value_counts()
fig = plt.figure(figsize=(18,6))
sns.barplot(x=counts._index, y=counts.values)
plt.title("Rating distribution")
plt.show()
```

```{python}
def undersampling(df):
    positive, negative, _ = df.polarity.value_counts()
    df_positive = df[df.polarity == 'positive']
    df_positive = df_positive.sample(negative, random_state=1)
    df_negative = df[df.polarity == 'negative']
    df = pd.concat([df_positive, df_negative])
    #df = df.sample(frac=1)
    return df
```

```{python}
new_dataReviews = undersampling(dataReviews)
```

```{python}
counts = new_dataReviews["polarity"].value_counts()
fig = plt.figure(figsize=(18,6))
sns.barplot(x=counts._index, y=counts.values)
plt.title("Rating distribution")
plt.show()
```

### Funzioni varie

```{python}
def flat_list(l):
    return  [item for sublist in l for item in sublist]
```

```{python}
def plot_common_tokens(tokens, title, n=20):
    sentences = (list(itertools.chain(tokens)))
    flat_sentences = flat_list(sentences)
    counts = Counter(flat_sentences)
    #print(counts.most_common(30))
    common_words = [word[0] for word in counts.most_common(n)]
    common_counts = [word[1] for word in counts.most_common(n)]
    fig = plt.figure(figsize=(18,6))
    sns.barplot(x=common_words, y=common_counts)
    plt.title(title)
    plt.show()
```

```{python}
def word_Cloud(sentences):
    flat_sentences = flat_list(sentences)
    unique_string=(" ").join(flat_sentences)

    wordcloud = WordCloud(background_color="white").generate(unique_string)
    plt.figure(figsize = (10, 8), facecolor = None) 
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()
```

### Tokenizzazione

```{python}
new_dataReviews['token']=new_dataReviews['body'].progress_apply(word_tokenize)
```

```{python}
#new_dataReviews['token'] #Name: token, Length: 130648, dtype: object
```

### Stopwords

```{python}
stop_words=nltk.corpus.stopwords.words('italian')
stop_words
```

```{python}
new_dataReviews["cleaned"] = new_dataReviews["token"].progress_apply(lambda sentence : [word for word in sentence if word.lower() not in stop_words])
```

### Punctuation

```{python}
punctuation = string.punctuation
punctuation = punctuation + "..."+ "''" + "``" + "--"
print(punctuation)
```

```{python}
new_dataReviews["cleaned"] = new_dataReviews["cleaned"].progress_apply(lambda sentence : [word for word in sentence if word not in punctuation])
```

### Numbers

```{python}
regex_numbers = r'(?:(?:\d+,?)+(?:\.?\d+)?)'
```

```{python}
new_dataReviews["cleaned"] = new_dataReviews["cleaned"].progress_apply(lambda sentence : [re.sub(regex_numbers,"",word) for word in sentence if re.sub(regex_numbers,"",word) != ""])
```

### Eliminazione token di lunghezza 1

```{python}
new_dataReviews["cleaned"] = new_dataReviews["cleaned"].progress_apply(lambda sentence : [word for word in sentence if len(word)> 1])
```

```{python}
#new_dataReviews.to_csv("dati_puliti.csv")
```

```{python}
plot_common_tokens(new_dataReviews['cleaned'],'Most Common Tokens used in Reviews')
```

```{python}
word_Cloud(new_dataReviews["cleaned"])
```

```{python}
sentences = (list(itertools.chain(new_dataReviews["cleaned"])))
flat_sentences = flat_list(sentences)
counts = Counter(flat_sentences)
counts.most_common()
```

### Stemming

```{python}
stemmer = SnowballStemmer("italian")
def stemming_token(sentence,stemmer):
    stem = []
    for elem in sentence:
        stem.append(stemmer.stem(elem))
    return stem
```

```{python}
new_dataReviews["stemming"]=[stemming_token(row["cleaned"], stemmer) for _, row in tqdm(new_dataReviews.iterrows())]
```

```{python}
#new_dataReviews = pd.read_csv("../data/dati_finali.csv", sep=",", index_col=0)
```

```{python}
### da fare solo se si legge il csv finale
```

```{python}
#def str_to_list(sentence):
#   return ast.literal_eval(sentence)
```

```{python}
#import ast

#new_dataReviews["stemming"] = new_dataReviews["stemming"].progress_apply(str_to_list)
```

```{python}
new_dataReviews
```

```{python}
len(new_dataReviews)
```

### CountVectorizer

```{python}
count_vect = CountVectorizer(stop_words=None, lowercase=True)
#lowercase = true -> Convert all characters to lowercase before tokenizing.
#stop_words = None -> If None, no stop words will be used
bow = count_vect.fit(new_dataReviews['stemming'].apply(lambda x: " ".join(x)))
```

```{python}
import pickle
with open('../model/bow.bin', 'wb') as f:
    pickle.dump(bow, f, pickle.HIGHEST_PROTOCOL)
#s = pickle.dumps(model)
```

```{python}
with open('../model/bow.bin', 'rb') as f:
    # The protocol version used is detected automatically, so we do not
    # have to specify it.
    bow = pickle.load(f)
```

```{python}
count_vect
```

```{python}
#bow.get_feature_names()[::2000]
```

```{python}
#X_train, X_test, y_train, y_test = train_test_split(bow, new_dataReviews['polarity'], test_size=0.2, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(new_dataReviews['stemming'].apply(lambda x: " ".join(x)), new_dataReviews['polarity'], test_size=0.2, random_state=1)    
```

```{python}
print("train size: ",len(X_train))
print("test size:",len(X_test))
```

```{python}
print("y train distribution:\n",y_train.value_counts())
print("y train distribution:\n",y_test.value_counts())
```

```{python}
model = LogisticRegression()
model.fit(bow.transform(X_train), y_train)
```

```{python}
predictions = model.predict(bow.transform(X_test))
```

```{python}
predictions
```

```{python}
from sklearn.metrics import accuracy_score, confusion_matrix
accuracy_score(y_test, predictions)
```

```{python}
confusion_matrix(y_test, predictions)
```

```{python}
scores = cross_val_score(model, bow.transform(new_dataReviews['stemming'].apply(lambda x: " ".join(x))), new_dataReviews['polarity'], cv=10)
```

```{python}
scores
```

```{python}
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2)) 
```

```{python}
scoring = ['precision_macro', 'recall_macro']
scores = cross_validate(model, bow.transform(new_dataReviews['stemming'].apply(lambda x: " ".join(x))), new_dataReviews['polarity'], scoring=scoring,
                        cv=10, return_train_score=False)
```

```{python}
scores
```

```{python}
print("Accuracy: %0.2f (+/- %0.2f)" % (scores['test_precision_macro'].mean(), scores['test_precision_macro'].std() * 2)) 
```

```{python}
# for save model in file.bin
'''with open('../model/model2.bin', 'wb') as f:
    pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)'''
```

### Use model with costum sentences

```{python}
def clean_sentence(sentence):
    tokens = word_tokenize(sentence)
    tokens_clean = []
    for word in tokens:
        if word.lower() not in stop_words and word.lower() not in punctuation and not word.isnumeric() and len(word)> 1:
            
            tokens_clean.append(stemmer.stem(word))
    return ' '.join(tokens_clean)
```

```{python}
sentence="belle le scarpe e le stringhe per il colore , perÃ² sono comode"
```

```{python}
clean_sentence(sentence)
```

```{python}
# codice per caricare il modello per convertire frasi in array
with open('../model/bow.bin', 'rb') as f:
    # The protocol version used is detected automatically, so we do not
    # have to specify it.
    bow = pickle.load(f)
```

```{python}
print(bow.transform([clean_sentence(sentence)]))
```

```{python}
# codice per caricare il modello di regressione logistica
with open('../model/model.bin', 'rb') as f:
    # The protocol version used is detected automatically, so we do not
    # have to specify it.
    model = pickle.load(f)
```

```{python}
model.predict(bow.transform([clean_sentence(sentence)]))
```

```{python}
#for load model
'''with open('../model/model.bin', 'rb') as f:
    # The protocol version used is detected automatically, so we do not
    # have to specify it.
    model = pickle.load(f)'''
```

### LDA

```{python}
#https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0
```

```{python}
prod = "B0058BXHWE" 
new_dataReviews_prodotto = new_dataReviews[new_dataReviews["product"]  == prod]
```

### unigrammi

```{python}
texts = new_dataReviews_prodotto['cleaned']
```

```{python}
# creo il dizionario
dictionary = corpora.Dictionary(texts) 
```

```{python}
# Term Document Frequency -> converto testo in array
corpus = [dictionary.doc2bow(text) for text in texts]
#corpus
```

```{python}
def compute_coherence_values(corpus, dictionary, num_topics, texts):
    
    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=num_topics, 
                                           random_state=1)
    
    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
    
    return coherence_model_lda.get_coherence()
```

```{python}
def optimal_topics(dictionary, corpus, texts, max_topic):
    count=1
    c_v = []
    for num_topics in range(1, max_topic):
        print(count)
        coherence =  compute_coherence_values(corpus, dictionary, num_topics, texts)
        
        c_v.append(coherence)
        count += 1
    x = range(1, max_topic)
    plt.plot(x, c_v)
    plt.xlabel("num_topics")
    plt.ylabel("Coherence score")
    plt.legend(("c_v"), loc='best')
    plt.show()

    return c_v
```

```{python}
num_topics = optimal_topics(dictionary=dictionary, corpus=corpus, texts=texts, max_topic=10)
```

```{python}
num_topics
```

```{python}
ntopics=num_topics.index(max(num_topics))+1
```

```{python}
ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=ntopics, id2word=dictionary)
```

```{python}
#ldamodel.save('ldamodel1.gensim')
```

```{python}
topics = ldamodel.print_topics()
```

```{python}
topics
```

```{python}
x = ldamodel.show_topics(num_topics=ntopics, num_words=15, formatted=False)
topics_words = [[wd[0] for wd in tp[1]] for tp in x]
```

```{python}
for sentence in topics_words:
    print(" ".join(sentence))
```

```{python}
# Visualize the topics
pyLDAvis.enable_notebook()
LDAvis_prepared = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
LDAvis_prepared
```

#### LDA su bigrammi

```{python}
text_data = []
for sentence in new_dataReviews_prodotto['cleaned']:
    bigram = list(nltk.bigrams(sentence))
    tokens = []
    for i in bigram:
        tokens.append((''.join([w + ' ' for w in i])).strip())
    text_data.append(tokens)
```

```{python}
text_data
```

```{python}
texts=text_data
dictionary = corpora.Dictionary(texts) 
corpus = [dictionary.doc2bow(text) for text in texts]
```

```{python}
num_topics = optimal_topics(dictionary=dictionary, corpus=corpus, texts=texts, max_topic=10)
```

```{python}
ntopics=num_topics.index(max(num_topics))+1
ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=ntopics, id2word=dictionary)
```

```{python}
topics = ldamodel.print_topics()
x = ldamodel.show_topics(num_topics=ntopics, num_words=15, formatted=False)
topics_words = [[wd[0] for wd in tp[1]] for tp in x]
for sentence in topics_words:
    print(sentence)
```

```{python}
ntopics
```

```{python}
pyLDAvis.enable_notebook()
LDAvis_prepared = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
LDAvis_prepared
```

### Sentimenti recensioni neutre

```{python}
dataReviews_neutre = pd.read_csv("../data/dati_ridotti.csv", sep=",", index_col=0)
```

```{python}
dataReviews_neutre = dataReviews_neutre.loc[dataReviews_neutre["rating"]==3]
```

```{python}
dataReviews_neutre
```

```{python}
dataReviews_neutre["polarity"] = dataReviews_neutre["body"].progress_apply(lambda sentence: model.predict(bow.transform([clean_sentence(sentence)])))
```

```{python}
dataReviews_neutre["polarity"]
```

```{python}
print("Proportion of review with score=3 that is positive: {}%".format(len(dataReviews_neutre[dataReviews_neutre.polarity == "positive"]) / len(dataReviews_neutre)*100))
print("Proportion of review with score=3 that is negative: {}%".format(len(dataReviews_neutre[dataReviews_neutre.polarity == "negative"]) / len(dataReviews_neutre)*100))
```

```{python}
dataReviews_neutre["polarity"].value_counts().plot(kind="bar")

```

### Prodotti nel tempo

```{python}
dataReviews = pd.read_csv("../data/dati_ridotti.csv", sep=",", index_col=0)
```

```{python}
# look for good examples
dataReviews.groupby('product').count().sort_values('_id', ascending=False).head(10)
```

```{python}
# GOOD EXAMPLES:
# B01ETRGE7M bello
# B00LPHUTOO altalena
# B01EWQ10D8 non male
# B0058BXHWE resegono
# ... non ne ho piu provati di quelli del box sopra
prod = "B0058BXHWE" 
dataReviews_prodotto = dataReviews[dataReviews["product"]  == prod]
len(dataReviews_prodotto)
```

```{python}
dataReviews_prodotto["date"] = dataReviews_prodotto["date"].progress_apply(lambda string: datetime.strptime(string, '%Y-%m-%d')) 
```

```{python}
dataReviews_prodotto=dataReviews_prodotto.assign(
    Period=dataReviews_prodotto.date.dt.to_period('M')
)

```

```{python}
dataReviews_prodotto["polarity"] = dataReviews_prodotto["body"].progress_apply(lambda sentence: model.predict(bow.transform([clean_sentence(sentence)]))[0])
```

```{python}
dataReviews_prodotto_month = dataReviews_prodotto.groupby(['Period']).progress_apply(lambda x: len(x.loc[x["polarity"]=="positive"])/len(x))

# use the rating intead of polarity
dataReviews_prodotto_month_rat = dataReviews_prodotto.groupby(['Period']).progress_apply(lambda x: len(x.loc[x["rating"]>3])/len(x))



```

```{python}
dataReviews_prodotto_month.head(20)
```

```{python}
dataReviews_prodotto_month.plot()
```

```{python}
dataReviews_prodotto_month_rat.plot()
```

```{python}
str(dataReviews_prodotto["date"])
```
