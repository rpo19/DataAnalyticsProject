---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
import itertools
from collections import Counter
import nltk
from datetime import datetime

import string
from nltk import wordpunct_tokenize



import re

#progress bar
from tqdm import tqdm, tqdm_notebook

# instantiate
tqdm.pandas(tqdm_notebook)
```

```{python}
dataReviewsChunk = pd.read_json('../data/reviews.json', lines=True, chunksize=10000)
```

```{python}

chunk_list = []  # append each chunk df here 

# Each chunk is in df format
for chunk in dataReviewsChunk:

    chunk_list.append(chunk)

```

```{python}
dataReviews = pd.concat(chunk_list)
```

### Exploratory Data Analysis

```{python}
print("Number of reviews not filtered:", len(dataReviews))
```

```{python}
def convert_int_value(int_value):
    return int_value["$numberInt"]
```

#### Reducing reviews

```{python}
dataReviews["rating"]=dataReviews["rating"].apply(convert_int_value).astype(int)
```

```{python}
dataReviews["helpful"]=dataReviews["helpful"].apply(convert_int_value).astype(int)
```

```{python}
dataReviews = dataReviews[dataReviews.helpful != 0]
```

```{python}
dataReviews = dataReviews[dataReviews.verified == True]
```

```{python}
counts = dataReviews["rating"].value_counts()
```

```{python}
print("Reduced reviews distribution [5-1] stars:", counts.values)
```

```{python}
x = counts._index
print(x)
y = counts.values
print(y)
```

```{python}
fig = plt.figure(figsize=(18,6))
sns.barplot(x=counts._index, y=counts.values)
plt.title("Rating distribution")
plt.show()
```

```{python}
print("Proportion of review with score=1: {}%".format(len(dataReviews[dataReviews.rating == 1]) / len(dataReviews)*100))
print("Proportion of review with score=2: {}%".format(len(dataReviews[dataReviews.rating == 2]) / len(dataReviews)*100))
print("Proportion of review with score=3: {}%".format(len(dataReviews[dataReviews.rating == 3]) / len(dataReviews)*100))
print("Proportion of review with score=4: {}%".format(len(dataReviews[dataReviews.rating == 4]) / len(dataReviews)*100))
print("Proportion of review with score=5: {}%".format(len(dataReviews[dataReviews.rating == 5]) / len(dataReviews)*100))
```

### Preparing dataset for sentiment analysis

```{python}
dataReviews = pd.read_csv('../data/dati_ridotti.csv', sep=',')
```

```{python}
dataReviews_POS = pd.DataFrame(dataReviews[dataReviews.rating > 3].sample(1000))
dataReviews_POS['target'] = 'POS'
dataReviews_POS['polarity'] = 'None'

```

```{python}
dataReviews_NEG = pd.DataFrame(dataReviews[dataReviews.rating < 3].sample(1000))
dataReviews_NEG['target'] = 'NEG'
dataReviews_NEG['polarity'] = 'None'
```

```{python}
dataReviews_NEUT = pd.DataFrame(dataReviews[dataReviews.rating == 3].sample(1000))
dataReviews_NEUT['target'] = 'NEUT'
dataReviews_NEUT['polarity'] = 'None'
```

```{python}
# export csv to do sentix in R
dataReviews_POS.to_csv('../data/dataReviews_POS.csv', index=False, sep=',')
dataReviews_NEG.to_csv('../data/dataReviews_NEG.csv', index=False, sep=',')
dataReviews_NEUT.to_csv('../data/dataReviews_NEUT.csv', index=False, sep=',')
```

### Execute SentimentR.R

```{python}
# EXECUTE R CODE
```

### Read reviews polarized

```{python}
# import csv after sentix in R
dataReviews_POS = pd.read_csv('../data/dataReviews_POS_pol.csv', sep=',').mask(dataReviews_POS.eq('None')).dropna()
dataReviews_NEG = pd.read_csv('../data/dataReviews_NEG_pol.csv', sep=',').mask(dataReviews_NEG.eq('None')).dropna()
dataReviews_NEUT = pd.read_csv('../data/dataReviews_NEUT_pol.csv', sep=',').mask(dataReviews_NEUT.eq('None')).dropna()
```

```{python}
# merging dataframes
dataReviews_pol = pd.concat([dataReviews_POS,dataReviews_NEG,dataReviews_NEUT], ignore_index=True)
# dataReviews_pol = pd.concat([dataReviews_POS,dataReviews_NEG], ignore_index=True)
```

```{python}
len(dataReviews_pol)
```

### Accuracy for each target

```{python}
print("% accuracy POS: ", len(dataReviews_POS.loc[dataReviews_POS.polarity.astype(float) > 0])/len(dataReviews_POS)*100)
```

```{python}
print("% accuracy NEG: ", len(dataReviews_POS.loc[dataReviews_NEG.polarity.astype(float) < 0])/len(dataReviews_NEG)*100)
```

```{python}
print("% 3 stars with positive polarities: ", len(dataReviews_NEUT.loc[dataReviews_NEUT.polarity.astype(float) > 0])/len(dataReviews_NEUT)*100)
```

### Compare distribution of polarity and ratings

```{python}
dataReviews_pol['polarity'].astype(float).hist()
```

```{python}
dataReviews_pol['rating'].hist()
```

### Compare distribution of a product

```{python}
# good examples
# B01ETRGE7M bello
# B00LPHUTOO altalena
# B01EWQ10D8 non male
# B0058BXHWE resegone
prod_id = 'B01ETRGE7M'
dataReviews_product = dataReviews.loc[dataReviews['product'] == prod_id]
```

```{python}
# export csv for sentix R
dataReviews_product.to_csv('../data/single_product.csv', sep=',')

```

```{python}
# EXECUTE R CODE with prod_id 
```

```{python}
# import csv after sentix in R
dataReviews_product_pol = pd.read_csv('../data/single_product_pol.csv', sep=',').mask(dataReviews_POS.eq('None')).dropna()
```

```{python}
dataReviews_product_pol
```

```{python}
dataReviews_product_pol['polarity'].plot()
```

```{python}
dataReviews_product_pol['rating'].plot()
```

### Rating and polarity of a product in time

```{python}
dataReviews_product_pol["date"] = dataReviews_product_pol["date"].progress_apply(lambda string: \
                                                                           datetime.strptime(string, '%Y-%m-%d')) 
```

```{python}
dataReviews_product_pol=dataReviews_product_pol.assign(
    period=dataReviews_product_pol.date.dt.to_period('M')
)
```

```{python}
dataReviews_product_pol_month = dataReviews_product_pol.groupby('period').mean()
```

```{python}
dataReviews_product_pol_month['polarity'].plot()
```

```{python}
dataReviews_product_pol_month['polarity'].plot()
```

### Sentix (DISCARD ALL FROM HERE TO THE END ?)

```{python}
from nltk.stem.snowball import SnowballStemmer
```

```{python}
stemmer = SnowballStemmer("italian")
def stemming_token(sentence,stemmer):
    stem = []
    for elem in sentence:
        stem.append(stemmer.stem(elem))
    return stem
```

```{python}
sentix = pd.read_csv("../data/sentix.csv", sep="\t", header=None)
sentix.columns=["lemma", "POS", "synset_ID", "score_1", "score_2", "polarity", "intensity"] 
sentix.head(1)
```

```{python}
sentix["lemma"] = sentix["lemma"].astype(str)
sentix["stemming"] = sentix["lemma"].apply(stemmer.stem)
sentix.head(5)
```

```{python}
sentix["polarity"] = sentix["polarity"]*5
sentix.head(1)

```

### Sentix for lexicon based sentiment analysis

```{python}
sentix_agg = sentix.loc[sentix["POS"]=="a"]

# add extra words
sentix_agg = sentix_agg.append({'stemming': stemmer.stem('reso'), 'polarity': -5, 'intensity': 1, 'synset_ID': '00048739'}, ignore_index = True)
sentix_agg = sentix_agg.append({'stemming': stemmer.stem('restituito'), 'polarity': -5, 'intensity': 1, 'synset_ID': '00048739'}, ignore_index = True)
sentix_agg = sentix_agg.append({'stemming': 'bel', 'polarity': 5, 'intensity': 1, 'synset_ID': '00217728'}, ignore_index = True)
```

```{python}
sentix = sentix.append({'stemming': stemmer.stem('reso'), 'polarity': -5, 'intensity': 1, 'synset_ID': '00048739'}, ignore_index = True)
sentix = sentix.append({'stemming': stemmer.stem('restituito'), 'polarity': -5, 'intensity': 1, 'synset_ID': '00048739'}, ignore_index = True)
sentix = sentix.append({'stemming': 'bel', 'polarity': 5, 'intensity': 1, 'synset_ID': '00217728'}, ignore_index = True)
```

```{python}
sentix_agg.iloc[len(sentix_agg)-1]
```

```{python}
dataReviews_POS["token"] = dataReviews_POS["body"].apply(word_tokenize)
dataReviews_POS["stemming"] = [stemming_token(row["token"], stemmer) for _, row in dataReviews_POS.iterrows()]
dataReviews_NEG["token"] = dataReviews_NEG["body"].apply(word_tokenize)
dataReviews_NEG["stemming"] = [stemming_token(row["token"], stemmer) for _, row in dataReviews_NEG.iterrows()]
```

```{python}
dataReviews_POS.head(1)
```

```{python}
dataReviews_NEG.head(1)
```

```{python}
import statistics 

def lexicon_based_score(sentence,df_sentix):
    sum_pol = 0
    count=0
    #print(sentence)
    context = None
    for elem in sentence:
        #polarity = df_sentix[df_sentix.stemming  == elem]['polarity']
        values = df_sentix[df_sentix.stemming  == elem][['polarity','intensity','synset_ID']]
        if not(values.empty):
            count += 1
            #print(elem)
            #print(polarity)
            #print(sum_pol)
            #if len(polarity >1):
                #print(type(polarity.iloc[0]))
                #print("polarity",polarity.value_counts())
            #sum_pol += polarity.value_counts().index[0]#statistics.mean(polarity)
            synset_values = values[values.synset_ID == context]           
            if synset_values.empty:
                synset_values = values.iloc[0]
                context = synset_values['synset_ID']
            else:
                synset_values =  synset_values.iloc[0]
            sum_pol += synset_values['polarity']*synset_values['intensity']
            
            #print(polarity)
            #print(statistics.mean(polarity))
            #else:
            #    sum_pol += polarity
    #print(sum_pol)
    #print("_________________________________________________________")
    if count != 0:
        return sum_pol/count
    return 0
```

```{python}
polarity = sentix[sentix.stemming  == stemmer.stem('subito')][['polarity','intensity','synset_ID']]
polarity.iloc[0]['polarity']*polarity.iloc[0]['intensity']
```

```{python}

```

#### Sentix on positive reviews

```{python}
dataReviews_POS["pol"] = dataReviews_POS["stemming"].progress_apply(lambda x: lexicon_based_score(x,sentix_agg))

```

```{python}
dataReviews_NEG["pol"] = dataReviews_NEG["stemming"].progress_apply(lambda x: lexicon_based_score(x,sentix_agg))

```

```{python}
print("% accuracy POS: ", len(dataReviews_POS.loc[dataReviews_POS.pol > 0])/len(dataReviews_POS)*100)
```

```{python}
print("% accuracy NEG: ", len(dataReviews_NEG.loc[dataReviews_NEG.pol < 0])/len(dataReviews_NEG)*100)
```

```{python}
dataReviews_NEG[dataReviews_NEG.pol > 0]['body'].values[0]
```

```{python}
stemmer.stem('pessim')
```

### Prodotti nel tempo

```{python}
dataReviews["product"]
```

```{python}
prod = "B00DDPI5NS"
#prod = "B07DMJPV31"
df_product = dataReviews[dataReviews["product"]  == prod]
df_product
```

```{python}
#delete day
df_product=df_product.assign(
    Period=df_product.date.dt.to_period('M')
)
df_product
```

```{python}
df_product['token']=df_product['body'].progress_apply(word_tokenize)
```

```{python}
df_product
```

```{python}
sub_data
```

```{python}
sub_data["stemming"]=[stemming_token(row["token"], stemmer) for _, row in sub_data.iterrows()]
df_product["stemming"] = [stemming_token(df_product["token"], stemmer) for _, row in df_product.iterrows()]
```

```{python}
df_product['lexicon_score_sentix']= df_product["stemming"].progress_apply(lambda x: lexicon_based_score(x,sentix_agg))
```

```{python}
#dataReviews["product"]
df_month = df_product[['Period','rating']].groupby(['Period']).mean()
df_month
```

```{python}
df_month.plot(style='.-')
```

```{python}
df_product [df_product["Period"]  == "2019-03"]
```

```{python}
# manage_date(df_product["Period"].iloc[0])
df_product["new_period"] = df_product["Period"].apply(manage_date)
df_product
```

```{python}
final_product_df = df_product[['new_period','rating']].groupby(['new_period']).mean()
final_product_df
```

```{python}
final_product_df.plot(style='.-')
```
